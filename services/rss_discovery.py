#!/usr/bin/env python3
"""
Extract RSS Discovery Service - RSS –ø–æ–∏—Å–∫ –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã
–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π Extract API —Å–∏—Å—Ç–µ–º—ã
"""
import os
import asyncio
import aiohttp
import feedparser
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Tuple
import json
import hashlib
from dotenv import load_dotenv
import sys

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.database import Database
from app_logging import get_logger

# Load environment variables
load_dotenv()


class ExtractRSSDiscovery:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π —á–µ—Ä–µ–∑ RSS –ª–µ–Ω—Ç—ã
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'pending'
    """
    
    def __init__(self):
        self.logger = get_logger('extract_system.rss_discovery')
        self.db = Database()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å RSS –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã
        self.rss_sources = self._load_rss_sources()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.batch_size = 10  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS
        self.default_days_back = 7  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–≤–µ—Ä—è–µ–º 7 –¥–Ω–µ–π –Ω–∞–∑–∞–¥
        
        self.logger.info(
            f"ExtractRSSDiscovery initialized with {len(self.rss_sources)} RSS sources"
        )
    
    def _load_rss_sources(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å RSS –∏–∑ sources_extract.json"""
        sources = []
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ sources_extract.json –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã
            sources_path = os.path.join(os.path.dirname(__file__), 'sources_extract.json')
            with open(sources_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å RSS
            for source in data['sources']:
                if source.get('rss_url'):
                    sources.append({
                        'id': source['id'],
                        'name': source['name'],
                        'url': source['url'],
                        'rss_url': source['rss_url']
                    })
                    
            self.logger.info(f"Loaded {len(sources)} RSS sources from sources_extract.json")
            
        except Exception as e:
            self.logger.error(f"Error loading RSS sources for Extract API: {e}")
            
        return sources
    
    def load_sources(self) -> List[Dict]:
        """–ü—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return self.rss_sources
    
    def _generate_article_id(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç article_id –∏–∑ URL (–ø–µ—Ä–≤—ã–µ 16 —Å–∏–º–≤–æ–ª–æ–≤ SHA256)"""
        return hashlib.sha256(url.encode()).hexdigest()[:16]
    
    def _resolve_google_redirect(self, url: str) -> str:
        """–†–µ–∑–æ–ª–≤–∏—Ç Google redirect —Å—Å—ã–ª–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
        if 'google.com/url' not in url:
            return url
            
        try:
            import urllib.parse
            parsed = urllib.parse.urlparse(url)
            params = urllib.parse.parse_qs(parsed.query)
            if 'url' in params:
                final_url = params['url'][0]
                return final_url
        except Exception as e:
            self.logger.debug(f"Could not parse Google redirect: {e}")
            
        return url
    
    def _is_blocked_domain(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–º–µ–Ω –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º (—Å–æ—Ü—Å–µ—Ç–∏ –∏ —Ç.–¥.)"""
        blocked_domains = {
            'youtube.com', 'www.youtube.com', 'youtu.be', 'm.youtube.com',
            'facebook.com', 'www.facebook.com', 'm.facebook.com', 'fb.com',
            'instagram.com', 'www.instagram.com',
            'twitter.com', 'www.twitter.com', 'x.com', 'www.x.com',
            'reddit.com', 'www.reddit.com', 'm.reddit.com',
            'linkedin.com', 'www.linkedin.com',
            'tiktok.com', 'www.tiktok.com',
            'telegram.org', 'www.telegram.org', 't.me',
            'discord.com', 'www.discord.com', 'discord.gg',
            'pinterest.com', 'www.pinterest.com',
            'snapchat.com', 'www.snapchat.com',
            'whatsapp.com', 'www.whatsapp.com'
        }
        
        try:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()
            
            # –£–±–∏—Ä–∞–µ–º www. –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            domain_clean = domain.replace('www.', '')
            
            return domain in blocked_domains or domain_clean in blocked_domains
        except Exception:
            return False
    
    async def fetch_rss_feed(self, session: aiohttp.ClientSession, source: Dict) -> Tuple[str, List[Dict]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç RSS –ª–µ–Ω—Ç—É
        
        Returns:
            Tuple[source_id, List[articles]]
        """
        source_id = source['id']
        rss_url = source['rss_url']
        articles = []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π last_parsed timestamp
            global_last_parsed = self.db.get_global_last_parsed()
            try:
                last_parsed = datetime.fromisoformat(global_last_parsed.replace('Z', '+00:00'))
                # Ensure timezone awareness
                if last_parsed.tzinfo is None:
                    last_parsed = last_parsed.replace(tzinfo=timezone.utc)
            except:
                # Fallback if parsing fails
                last_parsed = datetime.now(timezone.utc) - timedelta(days=self.default_days_back)
            
            self.logger.debug(
                f"Fetching RSS for {source_id}: {rss_url}, last_parsed: {last_parsed.isoformat()}"
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º RSS
            async with session.get(rss_url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                if response.status != 200:
                    self.logger.warning(
                        f"RSS fetch failed for {source_id}: status {response.status}, url: {rss_url}"
                    )
                    return source_id, []
                
                content = await response.text()
            
            # –ü–∞—Ä—Å–∏–º RSS
            feed = feedparser.parse(content)
            
            if feed.bozo:
                self.logger.warning(
                    f"RSS parse warning for {source_id}: {str(feed.bozo_exception)}"
                )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å–∏ (–º–∞–∫—Å–∏–º—É–º 5 –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫)
            articles_found = 0
            MAX_ARTICLES_PER_SOURCE = 5
            
            for entry in feed.entries:
                if articles_found >= MAX_ARTICLES_PER_SOURCE:
                    break
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                published = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Ç–∞—Ç—å–∏
                if published and published <= last_parsed:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
                url = entry.get('link', '')
                if not url:
                    continue
                
                # –†–µ–∑–æ–ª–≤–∏–º Google redirects –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã
                final_url = self._resolve_google_redirect(url)
                if final_url != url:
                    self.logger.info(f"Extracted real URL: {final_url} from redirect: {url[:100]}...")
                
                if self._is_blocked_domain(final_url):
                    self.logger.debug(f"Skipped blocked domain: {final_url[:100]}")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ –≤ –ë–î (–∏—Å–ø–æ–ª—å–∑—É–µ–º final_url –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏)
                if self.db.article_exists(final_url):
                    self.logger.debug(f"Article already exists: {final_url[:100]}")
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                title = entry.get('title', '').strip()
                description = entry.get('summary', '').strip()
                
                if not title:
                    self.logger.warning(f"Skipping article without title: {url}")
                    continue
                
                articles.append({
                    'article_id': self._generate_article_id(final_url),
                    'source_id': source_id,
                    'url': final_url,  # Use extracted real URL instead of redirect
                    'title': title,
                    'description': description,
                    'published_date': published,
                    'content_status': 'pending',
                    'discovered_via': 'rss'
                })
                articles_found += 1
            
            self.logger.info(
                f"RSS discovery for {source_id}: {len(feed.entries)} entries processed, "
                f"{len(articles)} new articles found, last_parsed: {last_parsed.isoformat()}"
            )
            
        except asyncio.TimeoutError:
            self.logger.error(f"RSS timeout for {source_id}: {rss_url}")
        except Exception as e:
            self.logger.error(
                f"RSS error for {source_id}: {str(e)} (URL: {rss_url})"
            )
        
        return source_id, articles
    
    async def discover_from_sources(self, source_ids: Optional[List[str]] = None, progress_tracker=None) -> Dict:
        """
        –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS –ª–µ–Ω—Ç
        
        Args:
            source_ids: –°–ø–∏—Å–æ–∫ ID –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–µ—Å–ª–∏ None - –≤—Å–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
            progress_tracker: Optional ParsingProgressTracker instance
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
        """
        # Import log_operation for dashboard updates
        from app_logging import log_operation
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        if source_ids:
            sources_to_process = [
                s for s in self.rss_sources 
                if s['id'] in source_ids
            ]
        else:
            sources_to_process = self.rss_sources
        
        if not sources_to_process:
            self.logger.warning("No RSS sources to process")
            return {'sources_processed': 0, 'articles_discovered': 0, 'articles_saved': 0}
        
        # Log RSS start for dashboard visibility
        log_operation(
            f"üîç RSS Discovery –Ω–∞—á–∞—Ç –¥–ª—è {len(sources_to_process)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
            phase="rss_discovery",
            operation_type="rss_start",
            details={
                'total_sources': len(sources_to_process),
                'source_ids': [s['id'] for s in sources_to_process]
            }
        )
        
        self.logger.info(
            f"Starting RSS discovery for {len(sources_to_process)} sources"
        )
        
        # Start RSS discovery phase tracking
        if progress_tracker:
            progress_tracker.start_phase('rss_discovery', len(sources_to_process))
        
        stats = {
            'sources_processed': 0,
            'articles_discovered': 0,
            'articles_saved': 0,
            'errors': 0,
            'new_articles': 0
        }
        
        # –°–æ–∑–¥–∞–µ–º HTTP —Å–µ—Å—Å–∏—é
        connector = aiohttp.TCPConnector(limit=10)
        async with aiohttp.ClientSession(connector=connector) as session:
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–≥—Ä—É–∑–∫–∏
            for i in range(0, len(sources_to_process), self.batch_size):
                batch = sources_to_process[i:i + self.batch_size]
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ–º RSS –ª–µ–Ω—Ç—ã –≤ –±–∞—Ç—á–µ
                tasks = [
                    self.fetch_rss_feed(session, source)
                    for source in batch
                ]
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for result in results:
                    if isinstance(result, Exception):
                        self.logger.error(f"Batch processing error: {result}")
                        stats['errors'] += 1
                        if progress_tracker:
                            progress_tracker.update_phase_progress('rss_discovery', {
                                'processed_feeds': 1,
                                'errors': 1
                            })
                        continue
                    
                    source_id, articles = result
                    stats['sources_processed'] += 1
                    stats['articles_discovered'] += len(articles)
                    
                    # Log progress for dashboard updates
                    source_name = next((s['name'] for s in self.rss_sources if s['id'] == source_id), source_id)
                    log_operation(
                        f"üì° {source_name}: –Ω–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π",
                        phase="rss_discovery", 
                        operation_type="source_processed",
                        details={
                            'source_id': source_id,
                            'source_name': source_name,
                            'articles_found': len(articles),
                            'progress': f"{stats['sources_processed']}/{len(sources_to_process)}"
                        }
                    )
                    
                    # Update progress tracker
                    if progress_tracker:
                        progress_tracker.update_source(source_id, source_name)
                        progress_tracker.update_phase_progress('rss_discovery', {
                            'processed_feeds': 1,
                            'total_articles_found': len(articles)
                        })
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å–∏ –≤ –ë–î
                    for article in articles:
                        try:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º insert_article –º–µ—Ç–æ–¥ –∏–∑ database.py
                            saved_id = self.db.insert_article(article)
                            if saved_id:
                                stats['articles_saved'] += 1
                                stats['new_articles'] += 1
                                if progress_tracker:
                                    progress_tracker.update_phase_progress('rss_discovery', {
                                        'new_articles_found': 1
                                    })
                            else:
                                # insert_article –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                                self.logger.debug(f"Article not saved (duplicate?): {article['url'][:100]}")
                        except Exception as e:
                            self.logger.error(
                                f"Error saving article: {str(e)} (URL: {article['url'][:100]})"
                            )
                            stats['errors'] += 1
                    
                    # Note: Global last_parsed is now managed centrally, not per-source
                    # This prevents the synchronization issues we had before
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
                if i + self.batch_size < len(sources_to_process):
                    await asyncio.sleep(1)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π last_parsed timestamp –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        if stats['new_articles'] > 0:
            try:
                current_time = datetime.now(timezone.utc)
                timestamp_str = current_time.strftime('%Y-%m-%dT%H:%M:%SZ')
                self.db.set_global_last_parsed(timestamp_str)
                self.logger.info(f"Updated global last_parsed to: {timestamp_str}")
            except Exception as e:
                self.logger.error(f"Error updating global last_parsed: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        self.logger.info(
            f"RSS discovery completed: {stats['sources_processed']} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, "
            f"{stats['new_articles']} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –Ω–∞–π–¥–µ–Ω–æ, "
            f"{stats['articles_saved']} —Å—Ç–∞—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ, "
            f"{stats['errors']} –æ—à–∏–±–æ–∫"
        )
        
        # Log completion for dashboard visibility
        log_operation(
            f"‚úÖ RSS Discovery –∑–∞–≤–µ—Ä—à–µ–Ω: {stats['new_articles']} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π",
            phase="rss_discovery",
            operation_type="rss_completed",
            details={
                'sources_processed': stats['sources_processed'],
                'articles_discovered': stats['articles_discovered'],
                'articles_saved': stats['articles_saved'],
                'new_articles': stats['new_articles'],
                'errors': stats['errors']
            }
        )
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ñ–∞–∑—ã
        self.logger.info("===== RSS DISCOVERY PHASE COMPLETED =====")
        
        # Complete RSS discovery phase
        if progress_tracker:
            progress_tracker.complete_phase('rss_discovery')
        
        return stats
    
    async def discover_with_playwright_fallback(self, source_ids: List[str]) -> Dict:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å fallback –Ω–∞ Playwright –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö RSS
        (–ë—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø–æ–∑–∂–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        """
        # Playwright fallback not implemented - RSS parsing handles errors gracefully
        return await self.discover_from_sources(source_ids)


async def test_rss_discovery():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç RSS Discovery –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
    
    discovery = RssDiscoveryService()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º –Ω–∞–±–æ—Ä–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    test_sources = ['openai', 'google_ai', 'microsoft_ai']
    
    print(f"Testing RSS Discovery on sources: {test_sources}")
    print(f"Available RSS sources: {len(discovery.rss_sources)}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º discovery
    stats = await discovery.discover_from_sources(test_sources)
    
    print("\nDiscovery Results:")
    print(f"Sources processed: {stats['sources_processed']}")
    print(f"Articles discovered: {stats['articles_discovered']}")
    print(f"Articles saved: {stats['articles_saved']}")
    print(f"Errors: {stats['errors']}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º pending —Å—Ç–∞—Ç—å–∏ –≤ –ë–î
    db = Database()
    pending_count = db.get_pending_articles_count()
    print(f"\nTotal pending articles in DB: {pending_count}")


if __name__ == "__main__":
    asyncio.run(test_rss_discovery())