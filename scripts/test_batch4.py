#!/usr/bin/env python3
"""
Batch Testing Script - Round 4
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —á–µ—Ä–µ–∑ Firecrawl
"""

import requests
import json
import sys
import re
import time
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ —Å–∏—Å—Ç–µ–º–µ
sys.path.insert(0, str(Path(__file__).parent.parent))
from core.config import Config

config = Config()

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è)
SOURCES_TO_TEST = [
    {
        "id": "writer",
        "name": "Writer Engineering Blog",
        "url": "https://writer.com/engineering/"
    },
    {
        "id": "google_research",
        "name": "Google Research Blog",
        "url": "https://research.google/blog/"
    },
    {
        "id": "google_cloud_ai",
        "name": "Google Cloud AI Blog",
        "url": "https://cloud.google.com/blog/products/ai-machine-learning"
    },
    {
        "id": "standardbots",
        "name": "Standard Bots Blog",
        "url": "https://standardbots.com/blog"
    },
    {
        "id": "abb_robotics",
        "name": "ABB Robotics News",
        "url": "https://new.abb.com/products/robotics/news-and-media"
    }
]

def fetch_content_firecrawl(url: str, timeout: int = 120000):
    """–ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π markdown —á–µ—Ä–µ–∑ Firecrawl API"""
    api_url = "https://api.firecrawl.dev/v1/scrape"
    headers = {
        "Authorization": f"Bearer {config.FIRECRAWL_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "url": url,
        "formats": ["markdown"],
        "timeout": timeout
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=data, timeout=150)
        
        if response.status_code == 200:
            result = response.json()
            if result.get("success"):
                markdown = result.get("data", {}).get("markdown", "")
                return markdown, None
            else:
                return None, f"API error: {result}"
        else:
            return None, f"HTTP Error {response.status_code}: {response.text}"
            
    except Exception as e:
        return None, f"Exception: {str(e)}"

def analyze_content_structure(content: str, source_id: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É markdown –∏ –∏—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏"""
    print(f"\nüîç ANALYZING CONTENT STRUCTURE FOR {source_id.upper()}")
    print(f"Content length: {len(content)} characters")
    print("=" * 60)
    
    # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å—Å—ã–ª–æ–∫
    link_patterns = [
        r'\[([^\]]+)\]\((https://[^)]+)\)',  # Markdown links
        r'<a[^>]*href="(https://[^"]+)"[^>]*>([^<]*)</a>',  # HTML links
    ]
    
    all_links = []
    for i, pattern in enumerate(link_patterns):
        matches = list(re.finditer(pattern, content, re.IGNORECASE))
        print(f"Pattern {i+1}: {len(matches)} matches")
        
        for match in matches[:10]:  # Show first 10
            groups = match.groups()
            if len(groups) >= 2:
                text, url = groups[0], groups[1]
                if len(groups) == 2 and pattern.startswith('<a'):  # HTML pattern  
                    text, url = groups[1], groups[0]
                print(f"  - [{text[:50]}...] -> {url}")
                all_links.append((text, url))
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –¥–æ–º–µ–Ω—É
    source_domain = SOURCES_TO_TEST[0]['url'].split('/')[2].replace('www.', '') if SOURCES_TO_TEST else ""
    relevant_links = []
    
    for text, url in all_links:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
        url_domain = url.split('/')[2].replace('www.', '') if '://' in url else ''
        
        if source_domain and source_domain in url_domain:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (—Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
            combined_text = (text + url).lower()
            if any(keyword in combined_text for keyword in 
                   ['blog', 'post', 'article', 'news', 'ai', 'ml', 'machine learning', 'engineering', 'tech']):
                relevant_links.append((text, url))
    
    print(f"\nüìä LINK ANALYSIS:")
    print(f"Total links found: {len(all_links)}")
    print(f"Relevant domain links: {len(relevant_links)}")
    
    if relevant_links:
        print(f"Sample relevant links:")
        for i, (text, url) in enumerate(relevant_links[:5], 1):
            print(f"  {i}. [{text[:50]}...] -> {url}")
    
    return relevant_links

def generate_patterns(source_id: str, url: str, relevant_links: list):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç patterns –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å—Å—ã–ª–æ–∫"""
    print(f"\nüéØ GENERATING PATTERNS FOR {source_id.upper()}")
    
    if not relevant_links:
        print("No relevant links found, using generic patterns")
        return [], [], []
    
    domain = url.split('/')[2] if '://' in url else ''
    base_domain = domain.replace('www.', '').replace('.', '\\.')
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—É—Ç–∏ URL
    url_paths = set()
    for text, link_url in relevant_links:
        if domain in link_url:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å
            parts = link_url.split('/')
            if len(parts) > 3:
                base_path = '/'.join(parts[:4])  # https://domain.com/path
                path_pattern = base_path.replace(domain, base_domain) + '/[^)]+'
                url_paths.add(path_pattern)
    
    # –°–æ–∑–¥–∞–µ–º patterns
    patterns = []
    for path_pattern in list(url_paths)[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 pattern
        pattern = f"\\]\\(({path_pattern})\\)"
        patterns.append(pattern)
    
    # Patterns –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
    title_extractions = [
        "\\[([^\\]]+)\\]\\(",
        "\\*\\*([^*]+)\\*\\*", 
        "### ([^\\n]+)",
        "## ([^\\n]+)"
    ]
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    exclude_urls = ["/contact", "/about", "/careers", "/pricing", "/page/", "/category/", "/tag/", "/#"]
    
    return patterns, title_extractions, exclude_urls

def test_patterns_on_content(patterns: list, title_extractions: list, content: str, source_name: str):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–µ patterns –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
    print(f"\nüß™ TESTING GENERATED PATTERNS FOR {source_name}")
    
    all_matches = []
    
    for i, pattern in enumerate(patterns, 1):
        print(f"\nPattern {i}: {pattern}")
        
        try:
            matches = list(re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE))
            print(f"   Found: {len(matches)} matches")
            
            for j, match in enumerate(matches[:5], 1):  # Show first 5
                groups = match.groups()
                if len(groups) >= 1:
                    url = groups[0]
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å title
                    title = "No title"
                    full_match = match.group(0)
                    
                    for title_pattern in title_extractions:
                        title_match = re.search(title_pattern, full_match)
                        if title_match:
                            title = title_match.group(1).strip()
                            break
                    
                    print(f"   {j}. [{title[:60]}{'...' if len(title) > 60 else ''}]")
                    print(f"      URL: {url}")
                    
                    all_matches.append({'title': title, 'url': url})
                    
        except re.error as e:
            print(f"   ‚ùå Regex error: {e}")
    
    return all_matches

def create_extractor_config(source_id: str, name: str, url: str, patterns: list, 
                          title_extractions: list, exclude_urls: list):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é extractor'–∞"""
    return {
        "name": name,
        "url": url,
        "patterns": patterns,
        "title_extraction": title_extractions,
        "title_cleanup": [
            f"^{name.split()[0]}:\\s*",
            "^\\*\\*|\\*\\*$",
            "\\s+$"
        ],
        "exclude_urls": exclude_urls,
        "status": "tested_real"
    }

def update_config_file(source_configs: dict):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç source_extractors.json —Å –Ω–æ–≤—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏"""
    config_path = Path(__file__).parent.parent / 'services' / 'source_extractors.json'
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º extractors
        for source_id, source_config in source_configs.items():
            config_data['extractors'][source_id] = source_config
            print(f"‚úÖ Updated config for {source_id}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Configuration updated in {config_path}")
        
    except Exception as e:
        print(f"‚ùå Failed to update config: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ STARTING BATCH 4 TESTING")
    print(f"Sources to test: {len(SOURCES_TO_TEST)}")
    print("=" * 80)
    
    updated_configs = {}
    
    for i, source in enumerate(SOURCES_TO_TEST, 1):
        print(f"\n\n{'='*80}")
        print(f"TESTING SOURCE {i}/{len(SOURCES_TO_TEST)}: {source['name']}")
        print(f"{'='*80}")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        print(f"üì• Fetching content from {source['url']}")
        content, error = fetch_content_firecrawl(source['url'], timeout=120000)
        
        if error:
            print(f"‚ùå Failed to fetch content: {error}")
            continue
            
        if not content:
            print(f"‚ùå No content received")
            continue
            
        if len(content) < 200:
            print(f"‚ùå Content too short: {len(content)} characters")
            continue
            
        print(f"‚úÖ Content fetched: {len(content)} characters")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        content_file = Path(__file__).parent / f"content_{source['id']}.md"
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"üíæ Content saved to {content_file}")
        
        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        relevant_links = analyze_content_structure(content, source['id'])
        
        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º patterns
        patterns, title_extractions, exclude_urls = generate_patterns(
            source['id'], source['url'], relevant_links
        )
        
        if not patterns:
            print(f"‚ö†Ô∏è  No patterns generated for {source['id']}")
            continue
        
        # 4. –¢–µ—Å—Ç–∏—Ä—É–µ–º patterns
        matches = test_patterns_on_content(patterns, title_extractions, content, source['name'])
        
        # 5. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if matches:
            config_entry = create_extractor_config(
                source['id'], source['name'], source['url'],
                patterns, title_extractions, exclude_urls
            )
            updated_configs[source['id']] = config_entry
            
            print(f"\n‚úÖ SUCCESS: {source['id']}")
            print(f"   Patterns: {len(patterns)}")
            print(f"   Matches found: {len(matches)}")
            print(f"   Unique URLs: {len(set(match['url'] for match in matches))}")
        else:
            print(f"\n‚ùå FAILED: {source['id']} - No matches found")
        
        # Rate limiting - –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if i < len(SOURCES_TO_TEST):
            print(f"‚è≥ Waiting 5 seconds before next source...")
            time.sleep(5)
    
    # 6. –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
    if updated_configs:
        print(f"\n\nüìù UPDATING CONFIGURATION FILE")
        print(f"Successfully tested sources: {len(updated_configs)}")
        update_config_file(updated_configs)
        
        print(f"\nüéâ BATCH 4 COMPLETED!")
        print(f"‚úÖ Tested sources: {list(updated_configs.keys())}")
    else:
        print(f"\n‚ùå BATCH 4 FAILED - No sources successfully configured")

if __name__ == "__main__":
    main()