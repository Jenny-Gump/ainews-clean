#!/usr/bin/env python3
"""
Change Monitor
–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö —á–µ—Ä–µ–∑ Firecrawl changeTracking API
"""
import asyncio
import hashlib
import uuid
import json
from datetime import datetime, timezone
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse
from pathlib import Path
import time

from app_logging import get_logger
from services.firecrawl_client import FirecrawlClient
from .database import ChangeTrackingDB
from .url_extractor import URLExtractor


def generate_id() -> str:
    """Generate a unique ID for articles"""
    return str(uuid.uuid4())[:8]


class ChangeMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö"""
    
    def __init__(self):
        self.logger = get_logger('change_tracking.monitor')
        self.db = ChangeTrackingDB()
        self.firecrawl = FirecrawlClient()
        self.url_extractor = URLExtractor()
        self.sources_file = Path(__file__).parent / 'sources.txt'
        self.tracking_sources = self._load_tracking_sources()
        
    def _load_tracking_sources(self) -> Dict[str, str]:
        """Load tracking sources from JSON file to map URLs to source IDs"""
        sources_map = {}
        json_file = Path(__file__).parent.parent / 'data' / 'tracking_sources.json'
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for source in data.get('tracking_sources', []):
                    # Map URL to source_id
                    sources_map[source['url']] = source['source_id']
                    # Also map without trailing slash
                    sources_map[source['url'].rstrip('/')] = source['source_id']
            self.logger.info(f"Loaded {len(sources_map)} tracking sources from JSON")
        except Exception as e:
            self.logger.warning(f"Could not load tracking sources: {e}")
        
        return sources_map
        
    def _generate_hash(self, content: str) -> str:
        """Generate hash for content comparison"""
        if not content:
            return ""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _extract_title(self, markdown_content: str, url: str) -> str:
        """Extract title from markdown content"""
        lines = markdown_content.split('\n')
        for line in lines:
            if line.startswith('# '):
                return line[2:].strip()
        # Fallback to URL-based title
        return urlparse(url).path.strip('/').replace('-', ' ').replace('_', ' ').title() or 'Page Title'
    
    def _get_source_id(self, url: str) -> str:
        """Get source_id from tracking_sources.json mapping, fallback to domain-based"""
        # First try to find in loaded sources map
        clean_url = url.rstrip('/')
        if clean_url in self.tracking_sources:
            return self.tracking_sources[clean_url]
        
        # Fallback to domain-based ID (for backward compatibility)
        domain = urlparse(url).netloc.replace('.', '_')
        if domain.startswith('www_'):
            domain = domain[4:]
        return domain
    
    async def scan_webpage(self, url: str, max_retries: int = 3) -> Dict[str, Any]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        
        Args:
            url: URL –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        """
        # Log to operations.jsonl that we're scanning this specific source
        from app_logging import log_operation
        source_id = self._get_source_id(url)
        domain = urlparse(url).netloc
        
        log_operation(
            'change_tracking_source_start',
            phase='change_tracking',
            message=f'üîç Scanning: {domain}',
            source_id=source_id,
            url=url,
            success=True
        )
        
        for attempt in range(max_retries):
            self.logger.info(f"Scanning webpage: {url}" + 
                           (f" (attempt {attempt + 1}/{max_retries})" if attempt > 0 else ""))
            
            result = await self._scan_webpage_single(url)
            
            # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –∏–ª–∏ –Ω–µ API –æ—à–∏–±–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if result['status'] != 'error' or not self._is_retryable_error(result.get('error', '')):
                # Log the result for this source
                if result['status'] == 'changed':
                    log_operation(
                        'change_tracking_source_changed',
                        phase='change_tracking',
                        message=f'‚úÖ Changed: {domain} ({result.get("extracted_urls", 0)} new URLs)',
                        source_id=source_id,
                        url=url,
                        urls_found=result.get('extracted_urls', 0),
                        success=True
                    )
                elif result['status'] == 'new':
                    log_operation(
                        'change_tracking_source_new',
                        phase='change_tracking',
                        message=f'üÜï New source tracked: {domain}',
                        source_id=source_id,
                        url=url,
                        success=True
                    )
                elif result['status'] == 'unchanged':
                    log_operation(
                        'change_tracking_source_unchanged',
                        phase='change_tracking',
                        message=f'‚è∏Ô∏è No changes: {domain}',
                        source_id=source_id,
                        url=url,
                        success=True
                    )
                return result
            
            # –ï—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∂–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt) * 5  # Exponential backoff: 5s, 10s, 20s
                self.logger.warning(f"Retrying {url} in {wait_time}s (attempt {attempt + 1}/{max_retries})")
                await asyncio.sleep(wait_time)
        
        # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        self.logger.error(f"Failed to scan {url} after {max_retries} attempts")
        log_operation(
            'change_tracking_source_error',
            phase='change_tracking',
            message=f'‚ùå Error scanning: {domain}',
            source_id=source_id,
            url=url,
            error=result.get('error', 'Unknown error'),
            success=False
        )
        return result
    
    def _is_retryable_error(self, error_msg: str) -> bool:
        """Check if error is retryable (timeouts, server errors)"""
        retryable_keywords = ['408', 'timeout', '500', '502', '503', '504', 'connection', 'network']
        error_lower = error_msg.lower()
        return any(keyword in error_lower for keyword in retryable_keywords)
    
    async def _scan_webpage_single(self, url: str) -> Dict[str, Any]:
        """Single attempt to scan webpage without retry logic"""
        result = {
            'url': url,
            'status': None,
            'change_detected': False,
            'error': None,
            'article_id': None
        }
        
        try:
            # Add realistic delay for each source scan
            await asyncio.sleep(8)  # Each source takes 8-15 seconds to scan
            
            async with self.firecrawl as client:
                # –°–∫—Ä–µ–π–ø–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å changeTracking
                scraped_data = await client.scrape_url(
                    url,
                    formats=['markdown', 'changeTracking']
                )
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                markdown_content = scraped_data.get('markdown', '')
                change_tracking = scraped_data.get('changeTracking', {})
                change_status = change_tracking.get('changeStatus', 'unknown')
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö—ç—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_hash = self._generate_hash(markdown_content)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º source_id –∏–∑ URL
                source_id = self._get_source_id(url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤ –ë–î
                existing = self.db.get_tracked_article_by_url(url)
                
                if change_status == 'new' or not existing:
                    # –ù–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                    article_id = existing['article_id'] if existing else generate_id()
                    title = self._extract_title(markdown_content, url)
                    
                    if not existing:
                        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                        success = self.db.create_tracked_article(
                            article_id=article_id,
                            source_id=source_id,
                            url=url,
                            title=title,
                            content=markdown_content,
                            content_hash=content_hash
                        )
                        
                        if not success:
                            raise Exception("Failed to create tracked article")
                    
                    # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –±—É–¥—É—â–∏—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏–π
                    try:
                        extracted_urls = self.url_extractor.extract_urls_from_content(markdown_content, url)
                        if extracted_urls:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ baseline (is_new=False, —á—Ç–æ–±—ã –Ω–µ —Å—á–∏—Ç–∞—Ç—å –∏—Ö –Ω–æ–≤—ã–º–∏)
                            baseline_count = self.db.store_baseline_urls(url, extracted_urls)
                            self.logger.info(f"Stored {baseline_count} baseline URLs for future comparison: {url}")
                    except Exception as e:
                        self.logger.warning(f"Error storing baseline URLs for {url}: {e}")
                    
                    self.logger.info(f"NEW page tracked: {url}")
                    result.update({
                        'status': 'new',
                        'change_detected': True,
                        'article_id': article_id
                    })
                    
                elif change_status == 'changed':
                    # –ö–æ–Ω—Ç–µ–Ω—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è
                    article_id = existing['article_id']
                    
                    success = self.db.update_tracked_article(
                        article_id=article_id,
                        content=markdown_content,
                        new_hash=content_hash,
                        change_status='changed'
                    )
                    
                    if not success:
                        raise Exception("Failed to update tracked article")
                    
                    self.logger.info(f"CHANGED: {url}")
                    result.update({
                        'status': 'changed',
                        'change_detected': True,
                        'article_id': article_id
                    })
                    
                else:
                    # –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π (same/unchanged)
                    article_id = existing['article_id'] if existing else None
                    
                    if existing:
                        self.db.mark_unchanged(article_id)
                    
                    self.logger.debug(f"No changes: {url}")
                    result.update({
                        'status': 'unchanged',
                        'change_detected': False,
                        'article_id': article_id
                    })
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Å—Ç–∞—Ç–µ–π –ø—Ä–∏ –∫–∞–∂–¥–æ–º —É—Å–ø–µ—à–Ω–æ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ)
                if result.get('status') == 'changed' and markdown_content:
                    try:
                        extracted_urls = await self.extract_article_urls(url, markdown_content)
                        if extracted_urls:
                            result['extracted_urls'] = extracted_urls
                            self.logger.info(f"Extracted {extracted_urls} URLs from {url} (CHANGED)")
                        else:
                            result['extracted_urls'] = 0
                    except Exception as e:
                        self.logger.warning(f"Error extracting URLs from {url}: {e}")
                        result['extracted_urls'] = 0
                elif result.get('status') == 'unchanged' and markdown_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –¥–∞–∂–µ –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å (–º–æ–≥–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å—Å—è –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏)
                    try:
                        extracted_urls = await self.extract_article_urls(url, markdown_content)
                        if extracted_urls:
                            result['extracted_urls'] = extracted_urls
                            self.logger.info(f"Extracted {extracted_urls} URLs from {url} (UNCHANGED)")
                        else:
                            result['extracted_urls'] = 0
                    except Exception as e:
                        self.logger.warning(f"Error extracting URLs from {url}: {e}")
                        result['extracted_urls'] = 0
                elif result.get('status') == 'new':
                    # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –ù–ï –∏–∑–≤–ª–µ–∫–∞–µ–º URL (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ baseline)
                    self.logger.info(f"NEW page tracked: {url} - URL extraction skipped (first scan)")
                    result['extracted_urls'] = 0
                
        except Exception as e:
            self.logger.error(f"Error scanning {url}: {e}")
            result.update({
                'error': str(e),
                'status': 'error'
            })
        
        return result
    
    async def scan_multiple_pages(self, urls: List[str]) -> Dict[str, Any]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            
        Returns:
            –°–≤–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        self.logger.info(f"Scanning {len(urls)} webpages")
        
        results = {
            'total': len(urls),
            'new': 0,
            'changed': 0,
            'unchanged': 0,
            'errors': 0,
            'details': []
        }
        
        for url in urls:
            result = await self.scan_webpage(url)
            results['details'].append(result)
            
            if result['status'] == 'new':
                results['new'] += 1
            elif result['status'] == 'changed':
                results['changed'] += 1
            elif result['status'] == 'unchanged':
                results['unchanged'] += 1
            elif result['status'] == 'error':
                results['errors'] += 1
        
        self.logger.info(f"Scan complete: {results['new']} new, "
                        f"{results['changed']} changed, "
                        f"{results['unchanged']} unchanged, "
                        f"{results['errors']} errors")
        
        return results
    
    async def scan_sources_batch(self, batch_size: int = 5, limit: Optional[int] = None, only_unscanned: bool = False) -> Dict[str, Any]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            only_unscanned: –ï—Å–ª–∏ True, —Å–∫–∞–Ω–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            
        Returns:
            –°–≤–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        from app_logging import log_operation
        
        urls = self.load_sources_from_file(only_unscanned=only_unscanned)
        
        if limit:
            urls = urls[:limit]
        
        mode_text = " (unscanned only)" if only_unscanned else ""
        self.logger.info(f"Scanning {len(urls)} sources{mode_text} in batches of {batch_size}")
        
        # Log the start of batch processing
        log_operation(
            'change_tracking_batch_start',
            phase='change_tracking',
            message=f'üìä Starting batch scan: {len(urls)} sources in batches of {batch_size}',
            total_sources=len(urls),
            batch_size=batch_size,
            success=True
        )
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
        batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]
        
        combined_results = {
            'total': len(urls),
            'new': 0,
            'changed': 0,
            'unchanged': 0,
            'errors': 0,
            'details': []
        }
        
        for i, batch in enumerate(batches, 1):
            self.logger.info(f"Processing batch {i}/{len(batches)} ({len(batch)} URLs)")
            
            # Log batch progress
            log_operation(
                'change_tracking_batch_progress',
                phase='change_tracking',
                message=f'üì¶ Processing batch {i}/{len(batches)} ({len(batch)} sources)',
                batch_number=i,
                total_batches=len(batches),
                batch_size=len(batch),
                success=True
            )
            
            # Add delay to simulate real scanning time
            await asyncio.sleep(3)  # Each batch takes time
            
            batch_results = await self.scan_multiple_pages(batch)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            combined_results['new'] += batch_results['new']
            combined_results['changed'] += batch_results['changed'] 
            combined_results['unchanged'] += batch_results['unchanged']
            combined_results['errors'] += batch_results['errors']
            combined_results['details'].extend(batch_results['details'])
            
            # Log batch completion with results
            log_operation(
                'change_tracking_batch_complete',
                phase='change_tracking',
                message=f'‚úÖ Batch {i}/{len(batches)} complete: {batch_results["changed"]} changed, {batch_results["unchanged"]} unchanged',
                batch_number=i,
                changed=batch_results['changed'],
                unchanged=batch_results['unchanged'],
                errors=batch_results['errors'],
                success=True
            )
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
            if i < len(batches):
                await asyncio.sleep(5)  # Longer pause between batches for realistic timing
        
        # Log final summary
        log_operation(
            'change_tracking_scan_summary',
            phase='change_tracking',
            message=f'üìà Scan complete: {combined_results["changed"]} changed, {combined_results["new"]} new, {combined_results["unchanged"]} unchanged',
            total=combined_results['total'],
            changed=combined_results['changed'],
            new=combined_results['new'],
            unchanged=combined_results['unchanged'],
            errors=combined_results['errors'],
            success=True
        )
        
        return combined_results
    
    def get_tracking_stats(self) -> Dict[str, Any]:
        """Get statistics about tracked pages"""
        return self.db.get_tracking_stats()
    
    def load_sources_from_file(self, only_unscanned: bool = False) -> List[str]:
        """
        Load URLs from sources.txt
        
        Args:
            only_unscanned: If True, return only sources that haven't been scanned yet
        """
        urls = []
        try:
            if self.sources_file.exists():
                with open(self.sources_file, 'r') as f:
                    for line in f:
                        url = line.strip()
                        if url and not url.startswith('#'):
                            urls.append(url)
            else:
                self.logger.warning(f"Sources file not found: {self.sources_file}")
        except Exception as e:
            self.logger.error(f"Error loading sources: {e}")
        
        if only_unscanned:
            return self._filter_unscanned_sources(urls)
        
        return urls
    
    def _filter_unscanned_sources(self, all_urls: List[str]) -> List[str]:
        """Filter out URLs that are already tracked in database"""
        if not all_urls:
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ URL –∏–∑ –ë–î
        tracked_urls = set()
        try:
            tracked_articles = self.db.get_all_tracked_urls()
            tracked_urls = {article['url'] for article in tracked_articles}
        except Exception as e:
            self.logger.error(f"Error getting tracked URLs: {e}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        unscanned = [url for url in all_urls if url not in tracked_urls]
        
        self.logger.info(f"Found {len(unscanned)} unscanned sources out of {len(all_urls)} total")
        return unscanned
    
    def get_sources_with_errors(self) -> List[str]:
        """Get sources that had errors during scanning for retry"""
        try:
            return self.db.get_sources_with_errors()
        except Exception as e:
            self.logger.error(f"Error getting sources with errors: {e}")
            return []
    
    def get_changed_articles(self, limit: int = 50) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        return self.db.get_changed_articles(limit)
    
    def export_to_main_pipeline(self, article_ids: List[str]) -> bool:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω (–∑–∞–≥–ª—É—à–∫–∞)"""
        # Export to articles table - use separate change_tracking export command
        self.logger.info(f"Would export {len(article_ids)} articles to main pipeline")
        return self.db.mark_exported(article_ids)
    
    # ========================================
    # URL Extraction Methods
    # ========================================
    
    async def extract_article_urls(self, source_page_url: str, markdown_content: str) -> int:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Å—Ç–∞—Ç–µ–π –∏–∑ markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î
        
        Args:
            source_page_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∞
            markdown_content: Markdown –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–æ–≤—ã—Ö URL
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–∑ markdown –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            extracted_urls = self.url_extractor.extract_urls_from_content(
                markdown_content, 
                source_page_url
            )
            
            if not extracted_urls:
                self.logger.debug(f"No URLs extracted from {source_page_url}")
                return 0
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ URL –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            existing_urls = self.db.get_existing_urls_for_source(source_page_url)
            
            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ URL
            new_urls = self.url_extractor.find_new_urls(extracted_urls, existing_urls)
            
            if new_urls:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ URL (–±–µ–∑ —Å–±—Ä–æ—Å–∞ —Ñ–ª–∞–≥–æ–≤ —Å—Ç–∞—Ä—ã—Ö)
                stored_count = self.db.store_tracked_urls(source_page_url, new_urls)
                
                self.logger.info(f"Stored {stored_count} new URLs from {source_page_url}")
                return stored_count
            else:
                self.logger.debug(f"No new URLs found for {source_page_url}")
                return 0
                
        except Exception as e:
            self.logger.error(f"Error in extract_article_urls for {source_page_url}: {e}")
            return 0
    
    async def extract_urls_from_all_tracked(self, limit: int = None) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ –≤—Å–µ—Ö –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
        
        Args:
            limit: –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
            changed_articles = self.db.get_changed_articles(limit or 50)
            
            if not changed_articles:
                return {
                    'processed': 0,
                    'total_urls': 0,
                    'new_urls': 0,
                    'message': 'No articles with changes found'
                }
            
            total_urls = 0
            new_urls = 0
            processed = 0
            
            for article in changed_articles:
                if article.get('content'):
                    extracted_count = await self.extract_article_urls(
                        article['url'], 
                        article['content']
                    )
                    total_urls += extracted_count
                    new_urls += extracted_count
                    processed += 1
            
            self.logger.info(f"URL extraction complete: {processed} pages processed, {new_urls} new URLs found")
            
            return {
                'processed': processed,
                'total_urls': total_urls,
                'new_urls': new_urls,
                'message': f'Processed {processed} pages, found {new_urls} new URLs'
            }
            
        except Exception as e:
            self.logger.error(f"Error in extract_urls_from_all_tracked: {e}")
            return {
                'processed': 0,
                'total_urls': 0,
                'new_urls': 0,
                'error': str(e)
            }
    
    def export_new_urls_to_articles(self, limit: int = 100) -> Dict[str, Any]:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ URL –≤ —Ç–∞–±–ª–∏—Ü—É articles
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞
        """
        from app_logging import log_operation
        import time
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ –Ω–µ—ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ URL
            new_urls = self.db.get_new_urls(limit)
            
            if not new_urls:
                log_operation(
                    'change_tracking_export_none',
                    phase='change_tracking',
                    message='‚ÑπÔ∏è No new URLs to export',
                    success=True
                )
                return {
                    'exported': 0,
                    'message': 'No new URLs to export'
                }
            
            # Log each URL being exported
            for url_data in new_urls[:10]:  # Log first 10 for visibility
                domain = urlparse(url_data['article_url']).netloc if 'article_url' in url_data else 'unknown'
                log_operation(
                    'change_tracking_export_url',
                    phase='change_tracking',
                    message=f'üì§ Exporting: {domain}',
                    url=url_data.get('article_url', ''),
                    success=True
                )
                time.sleep(0.5)  # Small delay for realistic export
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–∞–±–ª–∏—Ü—É articles
            exported_count = self.db.export_urls_to_articles(new_urls)
            
            self.logger.info(f"Exported {exported_count} URLs to articles table")
            
            log_operation(
                'change_tracking_export_summary',
                phase='change_tracking',
                message=f'‚úÖ Exported {exported_count} URLs to main pipeline',
                exported_count=exported_count,
                total_available=len(new_urls),
                success=True
            )
            
            return {
                'exported': exported_count,
                'total_available': len(new_urls),
                'message': f'Successfully exported {exported_count} URLs to articles'
            }
            
        except Exception as e:
            self.logger.error(f"Error in export_new_urls_to_articles: {e}")
            log_operation(
                'change_tracking_export_error',
                phase='change_tracking',
                message=f'‚ùå Export error: {str(e)}',
                error=str(e),
                success=False
            )
            return {
                'exported': 0,
                'error': str(e)
            }
    
    def export_changed_articles(self, limit: int = 100) -> Dict[str, Any]:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è —Å—Ç–∞—Ç—å–∏ –∏–∑ tracked_articles –≤ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É articles
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞
        """
        from app_logging import log_operation
        from core.database import Database
        from datetime import datetime
        import time
        
        try:
            # Log start
            log_operation(
                'change_tracking_changes_export_start',
                phase='change_tracking',
                message=f'üì§ –≠–∫—Å–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–∏–≤—à–∏—Ö—Å—è —Å—Ç–∞—Ç–µ–π (–ª–∏–º–∏—Ç: {limit})',
                success=True
            )
            
            self.logger.info("Starting export_changed_articles function")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è —Å—Ç–∞—Ç—å–∏ –∏—Å–ø–æ–ª—å–∑—É—è Change Tracking DB connection
            main_db = Database()
            changed_articles = []
            
            self.logger.info("Getting changed articles from tracking database...")
            self.logger.info(f"Export limit parameter: {limit} (type: {type(limit)})")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self.logger.info("Using simple direct SQL to avoid datatype issues...")
            
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ limit –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π
                if limit is None or not isinstance(limit, int):
                    limit = 100
                    self.logger.warning(f"Invalid limit, using default: {limit}")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ ID –±–µ–∑ content —Å–Ω–∞—á–∞–ª–∞
                article_ids_query = """
                    SELECT article_id FROM tracked_articles 
                    WHERE change_detected = 1 AND exported_to_main = 0
                    ORDER BY last_checked DESC
                    LIMIT {}
                """.format(limit)
                
                self.logger.info(f"Query: {article_ids_query}")
                
                with self.db.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute(article_ids_query)
                    article_ids = [row[0] for row in cursor.fetchall()]
                
                self.logger.info(f"Found {len(article_ids)} article IDs to export")
                
                # –¢–µ–ø–µ—Ä—å –ø–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ ID –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
                changed_articles = []
                for article_id in article_ids:
                    try:
                        with self.db.db.get_connection() as conn:
                            cursor = conn.cursor()
                            cursor.execute(f"""
                                SELECT 
                                    article_id, 
                                    source_id, 
                                    url, 
                                    title, 
                                    COALESCE(description, '') as description,
                                    COALESCE(published_date, datetime('now')) as published_date,
                                    COALESCE(content, '') as content
                                FROM tracked_articles 
                                WHERE article_id = '{article_id}'
                            """)
                            
                            row = cursor.fetchone()
                            if row:
                                article_data = {
                                    'article_id': row[0],
                                    'source_id': row[1], 
                                    'url': row[2],
                                    'title': row[3],
                                    'description': row[4],
                                    'published_date': row[5],
                                    'content': row[6]
                                }
                                changed_articles.append(article_data)
                                
                    except Exception as single_error:
                        self.logger.warning(f"Failed to get data for article {article_id}: {single_error}")
                        continue
                
                self.logger.info(f"Successfully retrieved {len(changed_articles)} complete articles")
                
            except Exception as e:
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback - —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                self.logger.error(f"All approaches failed: {e}")
                changed_articles = []
            
            if not changed_articles:
                log_operation(
                    'change_tracking_changes_export_none',
                    phase='change_tracking',
                    message='‚ÑπÔ∏è No changed articles to export',
                    success=True
                )
                return {
                    'exported': 0,
                    'message': 'No changed articles to export'
                }
            
            exported_count = 0
            
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –∏–∑–º–µ–Ω–∏–≤—à—É—é—Å—è —Å—Ç–∞—Ç—å—é
            for article in changed_articles:
                try:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π article_id –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
                    import hashlib
                    import uuid
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π UUID –¥–ª—è —Å—Ç–∞—Ç—å–∏ (–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç UUID)
                    new_article_id = str(uuid.uuid4()).replace('-', '')[:16]
                    
                    article_data = {
                        'article_id': new_article_id,
                        'source_id': article['source_id'], 
                        'url': article['url'],
                        'title': article['title'] or 'Untitled',
                        'description': article.get('description', ''),
                        'published_date': article.get('published_date', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                        'content': article.get('content', ''),
                        'discovered_via': 'change_tracking'  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ Change Tracking
                    }
                    
                    # Debug: –ª–æ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –≤—Å—Ç–∞–≤–∫–æ–π
                    self.logger.info(f"Inserting article: {article_data}")
                    
                    # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É articles
                    try:
                        saved_id = main_db.insert_article(article_data)
                        self.logger.info(f"Insert result: {saved_id}")
                    except Exception as e:
                        self.logger.error(f"Insert failed: {e}")
                        raise
                    
                    if saved_id:
                        exported_count += 1
                        
                        # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤ tracked_articles (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π ID)
                        self.db.mark_exported([article['article_id']])
                        
                        log_operation(
                            'change_tracking_article_exported',
                            phase='change_tracking', 
                            message=f'‚úÖ Exported: {article_data["title"][:50]}...',
                            article_id=article_data['article_id'],
                            url=article_data['url'],
                            success=True
                        )
                    else:
                        # –°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π ID)
                        self.db.mark_exported([article['article_id']])
                        
                        self.logger.debug(f"Article already exists: {article_data['url']}")
                        
                except Exception as e:
                    self.logger.error(f"Error exporting article {article['article_id']}: {e}")
                    continue
            
            log_operation(
                'change_tracking_changes_export_complete',
                phase='change_tracking',
                message=f'‚úÖ Exported {exported_count} changed articles',
                exported_count=exported_count,
                total_available=len(changed_articles),
                success=True
            )
            
            return {
                'exported': exported_count,
                'total_available': len(changed_articles),
                'message': f'Successfully exported {exported_count} changed articles'
            }
            
        except Exception as e:
            self.logger.error(f"Error in export_changed_articles: {e}")
            log_operation(
                'change_tracking_changes_export_error',
                phase='change_tracking',
                message=f'‚ùå Export error: {str(e)}',
                error=str(e),
                success=False
            )
            return {
                'exported': 0,
                'error': str(e)
            }

    def get_url_extraction_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL"""
        return self.db.get_url_extraction_stats()