#!/usr/bin/env python3
"""
Batch Testing Script - Round 3
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Ä–µ–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —á–µ—Ä–µ–∑ Firecrawl
"""

import requests
import json
import sys
import re
import time
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ —Å–∏—Å—Ç–µ–º–µ
sys.path.insert(0, str(Path(__file__).parent.parent))
from core.config import Config

config = Config()

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
SOURCES_TO_TEST = [
    {
        "id": "databricks_tracking",
        "name": "Databricks AI Blog",
        "url": "https://www.databricks.com/blog/category/engineering/ai"
    },
    {
        "id": "stanford_ai", 
        "name": "Stanford AI Lab",
        "url": "https://ai.stanford.edu/"
    },
    {
        "id": "suno",
        "name": "Suno AI Blog", 
        "url": "https://suno.com/blog"
    },
    {
        "id": "waymo",
        "name": "Waymo Blog",
        "url": "https://waymo.com/blog/"
    },
    {
        "id": "tempus",
        "name": "Tempus Tech Blog",
        "url": "https://www.tempus.com/tech-blog/"
    }
]

def fetch_content_firecrawl(url: str, timeout: int = 60000):
    """–ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π markdown —á–µ—Ä–µ–∑ Firecrawl API"""
    api_url = "https://api.firecrawl.dev/v1/scrape"
    headers = {
        "Authorization": f"Bearer {config.FIRECRAWL_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "url": url,
        "formats": ["markdown"],
        "timeout": timeout
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=data, timeout=120)
        
        if response.status_code == 200:
            result = response.json()
            if result.get("success"):
                markdown = result.get("data", {}).get("markdown", "")
                return markdown, None
            else:
                return None, f"API error: {result}"
        else:
            return None, f"HTTP Error {response.status_code}: {response.text}"
            
    except Exception as e:
        return None, f"Exception: {str(e)}"

def analyze_content_structure(content: str, source_id: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É markdown –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç patterns"""
    print(f"\nüîç ANALYZING CONTENT STRUCTURE FOR {source_id.upper()}")
    print(f"Content length: {len(content)} characters")
    print("=" * 60)
    
    # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å—Å—ã–ª–æ–∫
    link_patterns = [
        r'\[([^\]]+)\]\((https://[^)]+)\)',  # Markdown links
        r'<a[^>]*href="(https://[^"]+)"[^>]*>([^<]*)</a>',  # HTML links
        r'https://[^\s\])\}]+',  # Plain URLs
    ]
    
    all_links = []
    for i, pattern in enumerate(link_patterns):
        matches = list(re.finditer(pattern, content, re.IGNORECASE))
        print(f"Pattern {i+1} ({pattern}): {len(matches)} matches")
        
        for match in matches[:5]:  # Show first 5
            groups = match.groups()
            if len(groups) >= 2:
                text, url = groups[0], groups[1]
                print(f"  - [{text[:50]}...] -> {url}")
                all_links.append((text, url))
            elif len(groups) == 1:
                url = groups[0] if groups[0].startswith('https://') else match.group(0)
                print(f"  - {url}")
                all_links.append(("", url))
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ 
    print(f"\nüìä LINK ANALYSIS:")
    print(f"Total links found: {len(all_links)}")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–æ–º–µ–Ω–∞–º
    domain_counts = {}
    relevant_links = []
    
    for text, url in all_links:
        domain = url.split('/')[2] if '://' in url else 'unknown'
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (—Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ AI/ML)
        if any(keyword.lower() in (text + url).lower() for keyword in 
               ['ai', 'ml', 'machine learning', 'neural', 'model', 'algorithm', 'data', 'blog', 'article', 'news']):
            relevant_links.append((text, url))
    
    print(f"Domains found:")
    for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {domain}: {count} links")
    
    print(f"Relevant links (with AI/ML keywords): {len(relevant_links)}")
    
    return relevant_links, domain_counts

def generate_patterns(source_id: str, url: str, relevant_links: list):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç patterns –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å—Å—ã–ª–æ–∫"""
    print(f"\nüéØ GENERATING PATTERNS FOR {source_id.upper()}")
    
    domain = url.split('/')[2] if '://' in url else ''
    base_domain = domain.replace('www.', '')
    
    patterns = []
    title_extractions = []
    exclude_urls = []
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    for text, link_url in relevant_links:
        if base_domain in link_url:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω URL
            url_pattern = link_url.replace(domain, base_domain.replace('.', '\\.'))
            url_pattern = re.sub(r'/[^/]+/?$', '/[^)]+', url_pattern)
            
            # –°–æ–∑–¥–∞–µ–º markdown link pattern
            if text:
                pattern = f"\\]\\(({url_pattern})\\)"
                patterns.append(pattern)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º pattern –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                if '**' in text:
                    title_extractions.append("\\*\\*([^*]+)\\*\\*")
                elif text:
                    title_extractions.append("\\[([^\\]]+)\\]\\(")
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    patterns = list(dict.fromkeys(patterns))
    title_extractions = list(dict.fromkeys(title_extractions))
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    exclude_urls = ["/contact", "/about", "/careers", "/pricing", "/page/", "/category/", "/tag/"]
    
    return patterns[:3], title_extractions[:2], exclude_urls  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

def create_extractor_config(source_id: str, name: str, url: str, patterns: list, 
                          title_extractions: list, exclude_urls: list):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é extractor'–∞"""
    return {
        "name": name,
        "url": url,
        "patterns": patterns,
        "title_extraction": title_extractions,
        "title_cleanup": [
            f"^{name.split()[0]}:\\s*",
            "^\\*\\*|\\*\\*$",
            "\\s+$"
        ],
        "exclude_urls": exclude_urls,
        "status": "tested_real"
    }

def test_patterns_on_content(patterns: list, title_extractions: list, content: str):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–µ patterns –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
    print(f"\nüß™ TESTING GENERATED PATTERNS")
    
    all_matches = []
    
    for i, pattern in enumerate(patterns, 1):
        print(f"\nPattern {i}: {pattern}")
        
        try:
            matches = list(re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE))
            print(f"   Found: {len(matches)} matches")
            
            for j, match in enumerate(matches[:5], 1):  # Show first 5
                groups = match.groups()
                if len(groups) >= 1:
                    url = groups[-1] if groups[-1].startswith('https://') else groups[0]
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å title
                    title = "No title"
                    for title_pattern in title_extractions:
                        title_match = re.search(title_pattern, match.group(0))
                        if title_match:
                            title = title_match.group(1).strip()
                            break
                    
                    print(f"   {j}. [{title[:60]}{'...' if len(title) > 60 else ''}]")
                    print(f"      URL: {url}")
                    
                    all_matches.append({'title': title, 'url': url})
                    
        except re.error as e:
            print(f"   ‚ùå Regex error: {e}")
    
    return all_matches

def update_config_file(source_configs: dict):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç source_extractors.json —Å –Ω–æ–≤—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏"""
    config_path = Path(__file__).parent.parent / 'services' / 'source_extractors.json'
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º extractors
        for source_id, source_config in source_configs.items():
            config_data['extractors'][source_id] = source_config
            print(f"‚úÖ Updated config for {source_id}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Configuration updated in {config_path}")
        
    except Exception as e:
        print(f"‚ùå Failed to update config: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ STARTING BATCH 3 TESTING")
    print(f"Sources to test: {len(SOURCES_TO_TEST)}")
    print("=" * 80)
    
    updated_configs = {}
    
    for i, source in enumerate(SOURCES_TO_TEST, 1):
        print(f"\n\n{'='*80}")
        print(f"TESTING SOURCE {i}/{len(SOURCES_TO_TEST)}: {source['name']}")
        print(f"{'='*80}")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        print(f"üì• Fetching content from {source['url']}")
        content, error = fetch_content_firecrawl(source['url'], timeout=120000)  # 2 minutes
        
        if error:
            print(f"‚ùå Failed to fetch content: {error}")
            continue
            
        if not content:
            print(f"‚ùå No content received")
            continue
            
        print(f"‚úÖ Content fetched: {len(content)} characters")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        content_file = Path(__file__).parent / f"content_{source['id']}.md"
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"üíæ Content saved to {content_file}")
        
        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        relevant_links, domain_counts = analyze_content_structure(content, source['id'])
        
        if not relevant_links:
            print(f"‚ö†Ô∏è  No relevant links found for {source['id']}")
            continue
        
        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º patterns
        patterns, title_extractions, exclude_urls = generate_patterns(
            source['id'], source['url'], relevant_links
        )
        
        if not patterns:
            print(f"‚ö†Ô∏è  No patterns generated for {source['id']}")
            continue
        
        # 4. –¢–µ—Å—Ç–∏—Ä—É–µ–º patterns
        matches = test_patterns_on_content(patterns, title_extractions, content)
        
        # 5. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if matches:
            config_entry = create_extractor_config(
                source['id'], source['name'], source['url'],
                patterns, title_extractions, exclude_urls
            )
            updated_configs[source['id']] = config_entry
            
            print(f"\n‚úÖ SUCCESS: {source['id']}")
            print(f"   Patterns: {len(patterns)}")
            print(f"   Matches found: {len(matches)}")
            print(f"   Unique URLs: {len(set(match['url'] for match in matches))}")
        else:
            print(f"\n‚ùå FAILED: {source['id']} - No matches found")
        
        # Rate limiting - –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if i < len(SOURCES_TO_TEST):
            print(f"‚è≥ Waiting 3 seconds before next source...")
            time.sleep(3)
    
    # 6. –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
    if updated_configs:
        print(f"\n\nüìù UPDATING CONFIGURATION FILE")
        print(f"Successfully tested sources: {len(updated_configs)}")
        update_config_file(updated_configs)
        
        print(f"\nüéâ BATCH 3 COMPLETED!")
        print(f"‚úÖ Tested sources: {list(updated_configs.keys())}")
    else:
        print(f"\n‚ùå BATCH 3 FAILED - No sources successfully configured")

if __name__ == "__main__":
    main()