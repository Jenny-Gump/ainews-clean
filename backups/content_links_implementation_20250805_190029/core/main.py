#!/usr/bin/env python3
"""
AI News Parser - Extract API System
–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –±–∞–∑–µ Firecrawl Extract API
–°—Ç—Ä–æ–≥–æ –ø–æ 1 —Å—Ç–∞—Ç—å–µ, —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
"""
import argparse
import sys
import time
from datetime import datetime
from dotenv import load_dotenv
import os
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment variables from .env file
load_dotenv()

from core.database import Database
from core.config import Config
from app_logging import configure_logging, get_logger, LogContext

# Services imports
from services.content_parser import ContentParser
from services.rss_discovery import ExtractRSSDiscovery
from services.media_processor import ExtractMediaDownloaderPlaywright
from services.wordpress_publisher import WordPressPublisher

# Optional monitoring (graceful fallback if not available)
try:
    from monitoring.integration import MonitoringIntegration
    MONITORING_AVAILABLE = True
    # Shared monitoring database instance to prevent multiple connections
    _shared_monitoring_db = None
except ImportError as e:
    MonitoringIntegration = None
    MONITORING_AVAILABLE = False
    _shared_monitoring_db = None

def get_shared_monitoring_db():
    """Get shared monitoring database instance"""
    global _shared_monitoring_db
    if _shared_monitoring_db is None and MONITORING_AVAILABLE:
        from monitoring.database import MonitoringDatabase
        monitoring_db_path = str(Path(__file__).parent.parent / "data" / "monitoring.db")
        _shared_monitoring_db = MonitoringDatabase(monitoring_db_path)
    return _shared_monitoring_db

def parse_arguments():
    """–ü–∞—Ä—Å–∏—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã"""
    parser = argparse.ArgumentParser(
        description='AI News Parser - Extract API —Å–∏—Å—Ç–µ–º–∞ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Extract API —Å–∏—Å—Ç–µ–º—ã:

üîÑ EXTRACT API –ü–ê–†–°–ò–ù–ì (—Å—Ç—Ä–æ–≥–æ –ø–æ 1 —Å—Ç–∞—Ç—å–µ):
  python extract_system/main_extract.py --rss-full           # –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª
  python extract_system/main_extract.py --rss-discover       # Phase 1: RSS –ø–æ–∏—Å–∫
  python extract_system/main_extract.py --parse-pending      # Phase 2: Extract –ø–∞—Ä—Å–∏–Ω–≥
  
üìä –£–ü–†–ê–í–õ–ï–ù–ò–ï:
  python extract_system/main_extract.py --list-sources       # –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
  python extract_system/main_extract.py --stats              # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
  python extract_system/main_extract.py --cleanup            # –û—á–∏—Å—Ç–∫–∞

üöÄ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Å–ø–æ—Å–æ–±: python extract_system/main_extract.py --rss-full
        """
    )
    
    # –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –¥–ª—è Extract API)
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        '--run-once',
        action='store_true',
        help='–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞'
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
    parser.add_argument(
        '--list-sources',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤'
    )
    parser.add_argument(
        '--stats',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç–∞—Ç—å—è–º –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º'
    )
    parser.add_argument(
        '--cleanup',
        action='store_true',
        help='–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Å—Ç–∞—Ç—å–∏ (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)'
    )
    parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30)'
    )
    
    # Extract API –ø–∞—Ä—Å–∏–Ω–≥ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π - –±–µ–∑ batch –ª–∏–º–∏—Ç–æ–≤)
    parser.add_argument(
        '--rss-discover',
        action='store_true',
        help='Phase 1: Discover new articles from RSS feeds (saves as pending)'
    )
    parser.add_argument(
        '--parse-pending',
        action='store_true',
        help='Phase 2: Parse content for pending articles using Extract API (—Å—Ç—Ä–æ–≥–æ –ø–æ 1)'
    )
    parser.add_argument(
        '--rss-full',
        action='store_true',
        help='Run full RSS cycle: discovery + extract parsing (—Å—Ç—Ä–æ–≥–æ –ø–æ 1)'
    )
    parser.add_argument(
        '--days-back',
        type=int, 
        default=7,
        help='Filter articles newer than N days (default: 7)'
    )
    
    # –ú–µ–¥–∏–∞-–æ–±—Ä–∞–±–æ—Ç–∫–∞ Extract API
    parser.add_argument(
        '--media-only',
        action='store_true',
        help='Download media for articles that have Extract API data'
    )
    parser.add_argument(
        '--media-stats',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –º–µ–¥–∏–∞-—Ñ–∞–π–ª–∞–º'
    )
    
    # WordPress publishing (Phase 4)
    parser.add_argument(
        '--wordpress-prepare',
        action='store_true',
        help='Phase 4: Prepare articles for WordPress publishing (translation and rewriting)'
    )
    parser.add_argument(
        '--wordpress-publish',
        action='store_true',
        help='Phase 5: Publish prepared articles to WordPress'
    )
    parser.add_argument(
        '--upload-media-to-wordpress',
        action='store_true',
        help='Upload media files to WordPress for published articles'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=10,
        help='Limit number of articles to process (default: 10)'
    )
    
    return parser.parse_args()


def setup_monitoring():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–æ–±—â–∞—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π)"""
    logger = get_logger('extract_system.main')
    monitoring = None
    
    if MONITORING_AVAILABLE:
        try:
            logger.debug("DEBUG: Creating MonitoringIntegration")
            monitoring = MonitoringIntegration()
            logger.info("Monitoring integration –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {e}")
            monitoring = None
    else:
        logger.info("–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ—Ç–∫–ª—é—á–µ–Ω - —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ")
    
    return monitoring


async def run_rss_discovery():
    """
    Phase 1: RSS Discovery –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç sources_extract.json
    """
    logger = get_logger('extract_system.discovery')
    
    with LogContext.operation("extract_rss_discovery", 
                             system="extract_api", 
                             phase=1):
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º RSS Discovery –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã")
        
        # Get monitoring integration if available
        progress_tracker = None
        if MONITORING_AVAILABLE:
            try:
                from monitoring.parsing_tracker import ParsingProgressTracker
                monitoring_db = get_shared_monitoring_db()
                if monitoring_db:
                    progress_tracker = ParsingProgressTracker(monitoring_db)
                    progress_tracker.start()
                    logger.info("Progress tracking enabled for RSS discovery")
                else:
                    logger.warning("Could not initialize shared monitoring database")
            except Exception as e:
                logger.warning(f"Could not initialize progress tracker: {e}")
        
        discovery = ExtractRSSDiscovery()
        stats = await discovery.discover_from_sources(progress_tracker=progress_tracker)
        
        # Stop progress tracker
        if progress_tracker:
            progress_tracker.stop()
        
        logger.info(f"RSS Discovery –∑–∞–≤–µ—Ä—à–µ–Ω: {stats}")
        logger.info("===== RSS DISCOVERY PHASE COMPLETED =====")
        return stats


async def run_extract_parsing():
    """
    Phase 2: Extract API –ø–∞—Ä—Å–∏–Ω–≥
    –°–¢–†–û–ì–û –ü–û 1 –°–¢–ê–¢–¨–ï —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π last_parsed
    """
    logger = get_logger('extract_system.parsing')
    
    with LogContext.operation("extract_api_parsing", 
                             system="extract_api", 
                             phase=2):
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º Extract API –ø–∞—Ä—Å–∏–Ω–≥ (—Å—Ç—Ä–æ–≥–æ –ø–æ 1 —Å—Ç–∞—Ç—å–µ)")
        
        # Get monitoring integration if available
        progress_tracker = None
        if MONITORING_AVAILABLE:
            try:
                from monitoring.parsing_tracker import ParsingProgressTracker
                monitoring_db = get_shared_monitoring_db()
                if monitoring_db:
                    progress_tracker = ParsingProgressTracker(monitoring_db)
                    progress_tracker.start()
                    logger.info("Progress tracking enabled for content parsing")
                else:
                    logger.warning("Could not initialize shared monitoring database")
            except Exception as e:
                logger.warning(f"Could not initialize progress tracker: {e}")
        
        async with ContentParser() as parser:
            # –ü–æ–ª—É—á–∞–µ–º pending —Å—Ç–∞—Ç—å–∏ —Å —É—á–µ—Ç–æ–º last_parsed - –Ø–í–ù–û –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            db = Database()
            pending_articles = []
            
            # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –±–ª–æ–∫ —Å —è–≤–Ω—ã–º –∑–∞–∫—Ä—ã—Ç–∏–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            logger.debug("DEBUG: Opening database connection to get pending articles")
            with db.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT article_id, source_id, url, title 
                    FROM articles 
                    WHERE content_status = 'pending'
                    ORDER BY created_at ASC
                """)
                pending_articles = cursor.fetchall()
                logger.debug("DEBUG: Got pending articles from database")
            # –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–∫—Ä—ã—Ç–æ –∑–¥–µ—Å—å
            
            logger.debug("DEBUG: Database connection closed, converting to list")
            pending_articles = [dict(row) for row in pending_articles]
            
            if not pending_articles:
                logger.info("–ù–µ—Ç pending —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
                if progress_tracker:
                    progress_tracker.stop()
                return {"processed": 0, "successful": 0, "failed": 0}
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(pending_articles)} pending —Å—Ç–∞—Ç–µ–π –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            logger.debug("DEBUG: IMMEDIATELY after finding pending articles - DB connection is closed")
            
            # Start content parsing phase
            logger.debug("DEBUG: About to start content parsing phase")
            if progress_tracker:
                logger.debug("DEBUG: Calling progress_tracker.start_phase")
                try:
                    progress_tracker.start_phase('content_parsing', len(pending_articles))
                    logger.debug("DEBUG: progress_tracker.start_phase completed successfully")
                except Exception as e:
                    logger.error(f"DEBUG ERROR: progress_tracker.start_phase failed: {e}")
                    raise
            else:
                logger.debug("DEBUG: No progress_tracker available")
            
            stats = {"processed": 0, "successful": 0, "failed": 0}
            
            # –°–¢–†–û–ì–û –ü–û 1 –°–¢–ê–¢–¨–ï
            logger.debug("DEBUG: Starting article processing loop")
            for article in pending_articles:
                logger.debug(f"DEBUG: Processing article {article['article_id']}")
                with LogContext.article(article_id=article['article_id'], 
                                      article_url=article['url'],
                                      article_title=article['title']):
                    try:
                        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏: {article['title'][:50]}...")
                        
                        # Update progress tracker
                        if progress_tracker:
                            logger.debug("DEBUG: About to call progress_tracker.update_source")
                            source_name = article.get('source_id', 'Unknown')
                            progress_tracker.update_source(article['source_id'], source_name)
                            logger.debug("DEBUG: progress_tracker.update_source completed")
                            
                            logger.debug("DEBUG: About to call progress_tracker.update_pipeline_stage")
                            progress_tracker.update_pipeline_stage('parsing')
                            logger.debug("DEBUG: progress_tracker.update_pipeline_stage completed")
                        
                        result = await parser.parse_single_article(
                            article_id=article['article_id'],
                            url=article['url'],
                            source_id=article['source_id']
                        )
                        
                        stats["processed"] += 1
                        if result.get("success"):
                            stats["successful"] += 1
                            logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {article['title'][:50]}")
                            
                            if progress_tracker:
                                progress_tracker.update_phase_progress('content_parsing', {
                                    'processed_articles': 1,
                                    'successful': 1
                                })
                        else:
                            stats["failed"] += 1
                            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {article['title'][:50]}")
                            
                            if progress_tracker:
                                progress_tracker.update_phase_progress('content_parsing', {
                                    'processed_articles': 1,
                                    'failed': 1
                                })
                        
                    except Exception as e:
                        stats["processed"] += 1
                        stats["failed"] += 1
                        logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {article['article_id']}: {e}")
                        
                        if progress_tracker:
                            progress_tracker.update_phase_progress('content_parsing', {
                                'processed_articles': 1,
                                'failed': 1
                            })
            
            # Complete content parsing phase
            if progress_tracker:
                progress_tracker.complete_phase('content_parsing')
                progress_tracker.stop()
            
            logger.info(f"Extract –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {stats}")
            logger.info("===== CONTENT PARSING PHASE COMPLETED =====")
            return stats


async def run_media_download():
    """
    Phase 3: –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–µ–¥–∏–∞ –¥–ª—è Extract API –¥–∞–Ω–Ω—ã—Ö
    """
    logger = get_logger('extract_system.media')
    
    with LogContext.operation("extract_media_download", 
                             system="extract_api", 
                             phase=3):
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–µ–¥–∏–∞ –¥–ª—è Extract API —Å—Ç–∞—Ç–µ–π")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Playwright –≤–µ—Ä—Å–∏—é –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        from services.media_processor import ExtractMediaDownloaderPlaywright
        
        async with ExtractMediaDownloaderPlaywright() as downloader:
            stats = await downloader.download_all_media()
        
        logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–µ–¥–∏–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {stats}")
        logger.info("===== MEDIA PROCESSING PHASE COMPLETED =====")
        return stats


async def run_full_extract_cycle():
    """
    –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª Extract API —Å–∏—Å—Ç–µ–º—ã:
    1. RSS Discovery
    2. Extract API –ø–∞—Ä—Å–∏–Ω–≥ (—Å—Ç—Ä–æ–≥–æ –ø–æ 1)
    3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–µ–¥–∏–∞
    """
    logger = get_logger('extract_system.full_cycle')
    
    with LogContext.operation("extract_full_cycle", 
                             system="extract_api", 
                             phases="all"):
        logger.info("–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ Extract API —Å–∏—Å—Ç–µ–º—ã")
        
        # Phase 1: RSS Discovery
        discovery_stats = await run_rss_discovery()
        
        # Phase 2: Extract API –ø–∞—Ä—Å–∏–Ω–≥
        parsing_stats = await run_extract_parsing()
        
        # Phase 3: –ú–µ–¥–∏–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
        media_stats = await run_media_download()
        
        total_stats = {
            "discovery": discovery_stats,
            "parsing": parsing_stats,
            "media": media_stats
        }
        
        logger.info(f"–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª Extract API –∑–∞–≤–µ—Ä—à–µ–Ω: {total_stats}")
        logger.info("===== FULL EXTRACT API CYCLE COMPLETED =====")
        return total_stats


async def run_wordpress_prepare(limit: int = 10):
    """
    Phase 4: WordPress preparation
    Translate and rewrite articles for WordPress publishing
    """
    logger = get_logger('extract_system.wordpress')
    
    with LogContext.operation("wordpress_prepare", 
                             system="extract_api", 
                             phase=4):
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É —Å—Ç–∞—Ç–µ–π –¥–ª—è WordPress (–ª–∏–º–∏—Ç: {limit})")
        
        # Initialize WordPress publisher
        config = Config()
        db = Database()
        
        # Validate configuration
        config_errors = config.validate_config()
        if config_errors:
            logger.error(f"–û—à–∏–±–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {config_errors}")
            return {"error": "Configuration errors", "details": config_errors}
        
        publisher = WordPressPublisher(config, db)
        
        try:
            stats = publisher.process_articles_batch(limit=limit)
            logger.info(f"WordPress –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
            logger.info("===== WORDPRESS PREPARATION PHASE COMPLETED =====")
            return stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ WordPress –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏: {e}")
            return {"error": str(e)}


async def run_wordpress_publish(limit: int = 5):
    """
    Phase 5: WordPress publishing
    Publish translated articles to WordPress
    """
    logger = get_logger('extract_system.wordpress_publish')
    
    with LogContext.operation("wordpress_publish", 
                             system="extract_api", 
                             phase=5):
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é —Å—Ç–∞—Ç–µ–π –≤ WordPress (–ª–∏–º–∏—Ç: {limit})")
        
        # Initialize WordPress publisher
        config = Config()
        db = Database()
        
        # Validate WordPress API configuration
        if not all([config.wordpress_api_url, 
                   config.wordpress_username, 
                   config.wordpress_app_password]):
            logger.error("WordPress API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
            return {
                "error": "WordPress API not configured",
                "details": "–ù–∞—Å—Ç—Ä–æ–π—Ç–µ WORDPRESS_API_URL, WORDPRESS_USERNAME –∏ WORDPRESS_APP_PASSWORD –≤ .env"
            }
        
        publisher = WordPressPublisher(config, db)
        
        try:
            stats = publisher.publish_to_wordpress(limit=limit)
            logger.info(f"WordPress –ø—É–±–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
            logger.info("===== WORDPRESS PUBLISHING PHASE COMPLETED =====")
            return stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ WordPress –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
            return {"error": str(e)}


async def run_upload_media_to_wordpress(limit: int = 10):
    """
    Upload media files to WordPress for published articles
    """
    logger = get_logger('extract_system.wordpress_media')
    
    with LogContext.operation("wordpress_media_upload", 
                             system="extract_api", 
                             phase="media"):
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤ –≤ WordPress (–ª–∏–º–∏—Ç: {limit})")
        
        # Initialize WordPress publisher
        config = Config()
        db = Database()
        
        # Validate WordPress API configuration
        if not all([config.wordpress_api_url, 
                   config.wordpress_username, 
                   config.wordpress_app_password]):
            logger.error("WordPress API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
            return {
                "error": "WordPress API not configured",
                "details": "–ù–∞—Å—Ç—Ä–æ–π—Ç–µ WORDPRESS_API_URL, WORDPRESS_USERNAME –∏ WORDPRESS_APP_PASSWORD –≤ .env"
            }
        
        publisher = WordPressPublisher(config, db)
        
        try:
            stats = publisher.upload_media_to_wordpress(limit=limit)
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
            logger.info("===== WORDPRESS MEDIA UPLOAD COMPLETED =====")
            return stats
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ–¥–∏–∞—Ñ–∞–π–ª–æ–≤: {e}")
            return {"error": str(e)}


def show_sources():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è Extract API —Å–∏—Å—Ç–µ–º—ã"""
    logger = get_logger('extract_system.sources')
    
    try:
        discovery = ExtractRSSDiscovery()
        sources = discovery.load_sources()
        
        logger.info(f"Extract API —Å–∏—Å—Ç–µ–º–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {len(sources)} —à—Ç.")
        
        # Log source information
        for source in sources:
            logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {source['id']}: {source['name']}", 
                       source_id=source['id'], 
                       source_name=source['name'],
                       rss_url=source.get('rss_url', 'N/A'))
        
        # Also provide user-friendly output
        logger.info(f"\nüìã –ò—Å—Ç–æ—á–Ω–∏–∫–∏ Extract API —Å–∏—Å—Ç–µ–º—ã ({len(sources)} —à—Ç.):\n" + 
                   "=" * 60 + "\n" +
                   "\n".join([f"üî∏ {source['id']}: {source['name']}\n   RSS: {source.get('rss_url', 'N/A')}" 
                             for source in sources]))
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")


def show_stats():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Extract API —Å–∏—Å—Ç–µ–º—ã"""
    logger = get_logger('extract_system.stats')
    
    try:
        db = Database()
        with db.get_connection() as conn:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—Ç–∞—Ç–µ–π
            articles_stats = conn.execute("""
                SELECT 
                    content_status,
                    COUNT(*) as count
                FROM articles 
                GROUP BY content_status
            """).fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ–¥–∏–∞
            media_stats = conn.execute("""
                SELECT 
                    COUNT(*) as total_media,
                    COUNT(CASE WHEN alt_text IS NOT NULL THEN 1 END) as with_metadata
                FROM media_files
            """).fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ related_links
            related_stats = conn.execute("""
                SELECT COUNT(*) as total_related_links
                FROM related_links
            """).fetchone()
            
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Extract API —Å–∏—Å—Ç–µ–º—ã:")
        print("=" * 50)
        
        print("–°—Ç–∞—Ç—å–∏ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º:")
        for stat in articles_stats:
            print(f"   {stat['content_status']}: {stat['count']}")
        
        if media_stats:
            media = media_stats[0]
            print(f"\nüì∏ –ú–µ–¥–∏–∞-—Ñ–∞–π–ª—ã:")
            print(f"   –í—Å–µ–≥–æ: {media['total_media']}")
            print(f"   –° –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏: {media['with_metadata']}")
        
        if related_stats:
            print(f"\nüîó –°–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏: {related_stats['total_related_links']}")
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")


def cleanup_old_articles(days=30):
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π (–æ–±—â–∞—è –ª–æ–≥–∏–∫–∞ —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π)"""
    logger = get_logger('extract_system.cleanup')
    
    try:
        db = Database()
        with db.get_connection() as conn:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ç—å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π
            result = conn.execute("""
                DELETE FROM articles 
                WHERE created_at < datetime('now', '-{} days')
            """.format(days))
            
            deleted_count = result.rowcount
            
        logger.info(f"Cleanup completed: removed {deleted_count} articles older than {days} days", 
                   deleted_count=deleted_count, cleanup_days=days)
        logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ç–µ–π —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {e}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Extract API —Å–∏—Å—Ç–µ–º—ã"""
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º extract_system
    configure_logging()
    
    logger = get_logger('extract_system.main')
    args = parse_arguments()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    monitoring = setup_monitoring()
    
    try:
        logger.info("–ó–∞–ø—É—Å–∫ Extract API —Å–∏—Å—Ç–µ–º—ã")
        
        if args.list_sources:
            show_sources()
        elif args.stats:
            show_stats()
        elif args.cleanup:
            cleanup_old_articles(days=args.days)
        elif args.rss_discover:
            await run_rss_discovery()
        elif args.parse_pending:
            await run_extract_parsing()
        elif args.media_only:
            await run_media_download()
        elif args.rss_full or args.run_once:
            await run_full_extract_cycle()
        elif args.wordpress_prepare:
            await run_wordpress_prepare(limit=args.limit)
        elif args.wordpress_publish:
            await run_wordpress_publish(limit=args.limit)
        elif args.upload_media_to_wordpress:
            await run_upload_media_to_wordpress(limit=args.limit)
        else:
            logger.error("No operation mode specified. Use --help for available options")
            logger.info("–ù–µ —É–∫–∞–∑–∞–Ω —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("Extract API —Å–∏—Å—Ç–µ–º–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ Extract API —Å–∏—Å—Ç–µ–º—ã: {e}")
        sys.exit(1)
    finally:
        logger.info("Extract API —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())