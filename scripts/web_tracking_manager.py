#!/usr/bin/env python3
"""
Web Tracking Manager
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü (–Ω–µ RSS!) –∏–∑ sources.txt
"""

import asyncio
import sys
from pathlib import Path
from typing import List, Dict
from urllib.parse import urlparse

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app_logging import get_logger
from core.database import Database
from services.web_monitor import WebMonitor


class WebTrackingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö"""
    
    def __init__(self):
        self.logger = get_logger('scripts.web_tracking_manager')
        self.db = Database()
        self.monitor = WebMonitor()
        self.sources_file = Path('/Users/skynet/Desktop/sources.txt')
    
    def load_web_sources(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç URL –∏–∑ sources.txt"""
        if not self.sources_file.exists():
            self.logger.error(f"File not found: {self.sources_file}")
            return []
        
        urls = []
        with open(self.sources_file, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and not url.startswith('#'):
                    urls.append(url)
        
        return urls
    
    def init_sources_in_db(self, urls: List[str]) -> Dict[str, int]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –ë–î"""
        stats = {'added': 0, 'updated': 0, 'skipped': 0}
        
        for url in urls:
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º source_id –∏–∑ URL
                parsed = urlparse(url)
                domain = parsed.netloc.replace('www.', '')
                source_id = domain.replace('.', '_')
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è
                name = domain.replace('.', ' ').title()
                if parsed.path and len(parsed.path) > 1:
                    path_part = parsed.path.strip('/').replace('/', ' - ').replace('-', ' ').title()
                    name = f"{name} - {path_part}"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                with self.db.get_connection() as conn:
                    cursor = conn.execute(
                        "SELECT source_id FROM sources WHERE source_id = ?",
                        (source_id,)
                    )
                    exists = cursor.fetchone()
                
                if not exists:
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
                    with self.db.get_connection() as conn:
                        conn.execute("""
                            INSERT INTO sources (
                                source_id, name, url, type,
                                has_rss, validation_status
                            ) VALUES (?, ?, ?, ?, ?, ?)
                        """, (
                            source_id, name, url, 'web',
                            0,  # has_rss = False (—ç—Ç–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã, –Ω–µ RSS)
                            'active'
                        ))
                    stats['added'] += 1
                    self.logger.info(f"‚úÖ Added source: {name}")
                else:
                    stats['skipped'] += 1
                    self.logger.debug(f"‚è≠Ô∏è Source exists: {name}")
                    
            except Exception as e:
                self.logger.error(f"Error processing {url}: {e}")
        
        return stats
    
    async def monitor_all_sources(self, limit: int = 10) -> Dict:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç –≤—Å–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ sources.txt"""
        urls = self.load_web_sources()
        
        if not urls:
            self.logger.error("No URLs found in sources.txt")
            return {'error': 'No URLs found'}
        
        self.logger.info(f"Monitoring {len(urls[:limit])} web pages...")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)
        results = await self.monitor.monitor_multiple_pages(urls[:limit])
        
        return results
    
    async def monitor_single_source(self, url: str) -> Dict:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç –æ–¥–Ω—É –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        self.logger.info(f"Monitoring single webpage: {url}")
        return await self.monitor.monitor_webpage(url)
    
    async def export_articles(self) -> Dict:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –æ—Å–Ω–æ–≤–Ω—É—é –ë–î"""
        return await self.monitor.export_to_main()
    
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats = {}
        
        with self.db.get_connection() as conn:
            # –í—Å–µ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
            cursor = conn.execute("SELECT COUNT(*) FROM tracked_articles")
            stats['total_tracked'] = cursor.fetchone()[0]
            
            # –ü–æ —Å—Ç–∞—Ç—É—Å–∞–º
            cursor = conn.execute("""
                SELECT change_status, COUNT(*) 
                FROM tracked_articles 
                GROUP BY change_status
            """)
            stats['by_status'] = dict(cursor.fetchall())
            
            # –ù–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
            cursor = conn.execute("""
                SELECT COUNT(*) FROM tracked_articles 
                WHERE change_detected = 1 AND exported_to_main = 0
            """)
            stats['pending_export'] = cursor.fetchone()[0]
            
            # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –±–µ–∑ RSS (–≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã)
            cursor = conn.execute("""
                SELECT COUNT(*) FROM sources 
                WHERE has_rss = 0 AND validation_status = 'active'
            """)
            stats['web_sources'] = cursor.fetchone()[0]
        
        return stats


async def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Web Tracking Manager')
    parser.add_argument('--init', action='store_true', 
                       help='Initialize web sources from sources.txt')
    parser.add_argument('--monitor', type=str, 
                       help='Monitor specific webpage URL')
    parser.add_argument('--monitor-all', action='store_true',
                       help='Monitor all sources from sources.txt')
    parser.add_argument('--limit', type=int, default=10,
                       help='Limit number of pages to monitor (default: 10)')
    parser.add_argument('--export', action='store_true',
                       help='Export found articles to main DB')
    parser.add_argument('--stats', action='store_true',
                       help='Show statistics')
    parser.add_argument('--full-cycle', action='store_true',
                       help='Run full cycle: monitor + export')
    
    args = parser.parse_args()
    
    manager = WebTrackingManager()
    
    if args.init:
        print("\nüìö Initializing web sources from sources.txt...")
        urls = manager.load_web_sources()
        
        if urls:
            print(f"Found {len(urls)} URLs in sources.txt")
            stats = manager.init_sources_in_db(urls)
            print(f"\n‚úÖ Results:")
            print(f"  Added: {stats['added']}")
            print(f"  Skipped: {stats['skipped']}")
        else:
            print("‚ùå No URLs found in sources.txt")
    
    elif args.monitor:
        print(f"\nüîç Monitoring webpage: {args.monitor}")
        results = await manager.monitor_single_source(args.monitor)
        
        print(f"\nüìä Results:")
        print(f"  Status: {results.get('status')}")
        print(f"  Articles found: {len(results.get('articles_found', []))}")
        print(f"  Changes detected: {len(results.get('changes_detected', []))}")
        
        if results.get('articles_found'):
            print(f"\nüì∞ Found articles:")
            for i, article in enumerate(results['articles_found'][:5], 1):
                print(f"  {i}. {article['title'][:60]}")
                print(f"     {article['url']}")
    
    elif args.monitor_all:
        print(f"\nüîç Monitoring all sources (limit: {args.limit})...")
        results = await manager.monitor_all_sources(limit=args.limit)
        
        print(f"\nüìä Summary:")
        print(f"  Pages monitored: {results['total_monitored']}")
        print(f"  Articles found: {results['total_articles']}")
        print(f"  Changes detected: {results['total_changes']}")
        print(f"  Errors: {results['total_errors']}")
        
        if results.get('details'):
            print(f"\nüìã Per page breakdown:")
            for detail in results['details'][:10]:
                if detail.get('status') == 'error':
                    print(f"  ‚ùå {detail['url']}: ERROR")
                else:
                    print(f"  ‚Ä¢ {detail['url'][:50]}...")
                    print(f"    Articles: {len(detail.get('articles_found', []))}, "
                          f"Changes: {len(detail.get('changes_detected', []))}")
    
    elif args.export:
        print("\nüì§ Exporting articles to main database...")
        results = await manager.export_articles()
        
        print(f"\n‚úÖ Export results:")
        print(f"  Exported: {results['exported']}")
        print(f"  Duplicates: {results['duplicates']}")
        print(f"  Errors: {results['errors']}")
    
    elif args.stats:
        print("\nüìä WEB TRACKING STATISTICS:")
        stats = manager.get_stats()
        
        print(f"\nüìÑ Tracked Pages:")
        print(f"  Total tracked: {stats.get('total_tracked', 0)}")
        print(f"  Pending export: {stats.get('pending_export', 0)}")
        print(f"  Web sources: {stats.get('web_sources', 0)}")
        
        if stats.get('by_status'):
            print(f"\nüìä By Status:")
            for status, count in stats['by_status'].items():
                print(f"  {status}: {count}")
    
    elif args.full_cycle:
        print("\nüîÑ Running full cycle: monitor + export")
        
        # Monitor
        print(f"\n1Ô∏è‚É£ Monitoring web pages (limit: {args.limit})...")
        monitor_results = await manager.monitor_all_sources(limit=args.limit)
        print(f"  Found {monitor_results['total_articles']} articles")
        print(f"  Detected {monitor_results['total_changes']} changes")
        
        # Export
        if monitor_results['total_articles'] > 0 or monitor_results['total_changes'] > 0:
            print("\n2Ô∏è‚É£ Exporting to main database...")
            export_results = await manager.export_articles()
            print(f"  Exported {export_results['exported']} articles")
        else:
            print("\n2Ô∏è‚É£ No articles to export")
        
        # Stats
        print("\n3Ô∏è‚É£ Final statistics:")
        stats = manager.get_stats()
        print(f"  Total tracked: {stats.get('total_tracked', 0)}")
        print(f"  Pending export: {stats.get('pending_export', 0)}")
    
    else:
        parser.print_help()


if __name__ == '__main__':
    asyncio.run(main())