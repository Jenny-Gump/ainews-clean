#!/usr/bin/env python3
"""
Initialize Tracking Sources
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ sources.txt –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
"""

import sys
import json
from pathlib import Path
from urllib.parse import urlparse

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app_logging import get_logger
from core.database import Database


def parse_sources_txt(file_path: str):
    """–ü–∞—Ä—Å–∏—Ç sources.txt –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    sources = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            url = line.strip()
            if url and not url.startswith('#'):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –∏–∑ URL
                parsed = urlparse(url)
                domain = parsed.netloc.replace('www.', '')
                path = parsed.path.strip('/')
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º source_id
                source_id = domain.replace('.', '_')
                if path:
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ ID –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                    path_part = path.replace('/', '_').replace('-', '_')
                    source_id = f"{source_id}_{path_part}"[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è
                name = domain.replace('.', ' ').title()
                if path:
                    path_title = path.replace('/', ' - ').replace('-', ' ').title()
                    name = f"{name} - {path_title}"
                
                # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å RSS URL (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                rss_url = ""
                if 'blog' in url:
                    rss_url = f"{url}/feed" if not url.endswith('/') else f"{url}feed"
                elif 'news' in url:
                    rss_url = f"{url}/rss" if not url.endswith('/') else f"{url}rss"
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
                    rss_url = f"{url}/rss.xml" if not url.endswith('/') else f"{url}rss.xml"
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                category = 'general'
                if 'research' in url or 'stanford' in url or 'mit.edu' in url:
                    category = 'ai_research'
                elif any(company in url for company in ['anthropic', 'openai', 'google', 'microsoft', 'meta']):
                    category = 'ai_companies'
                elif 'cloud' in url or 'aws' in url:
                    category = 'cloud_ai'
                elif 'hugging' in url or 'databricks' in url or 'scale' in url:
                    category = 'ai_platforms'
                elif 'robot' in url or 'waymo' in url:
                    category = 'ai_robotics'
                elif 'health' in url or 'medical' in url or 'pathai' in url:
                    category = 'ai_healthcare'
                
                sources.append({
                    'source_id': source_id,
                    'name': name,
                    'url': url,
                    'rss_url': rss_url,
                    'type': 'web',
                    'category': category
                })
    
    return sources


def main():
    logger = get_logger('scripts.init_tracking_sources')
    db = Database()
    
    # –ü—É—Ç—å –∫ sources.txt
    sources_txt = Path('/Users/skynet/Desktop/sources.txt')
    
    if not sources_txt.exists():
        logger.error(f"File not found: {sources_txt}")
        return
    
    # –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    logger.info(f"Parsing {sources_txt}...")
    sources = parse_sources_txt(sources_txt)
    logger.info(f"Found {len(sources)} sources")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ë–î
    added = 0
    updated = 0
    
    for source in sources:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
            with db.get_connection() as conn:
                cursor = conn.execute(
                    "SELECT source_id, rss_url FROM sources WHERE source_id = ?",
                    (source['source_id'],)
                )
                existing = cursor.fetchone()
            
            if not existing:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
                with db.get_connection() as conn:
                    conn.execute("""
                        INSERT INTO sources (
                            source_id, name, url, type, has_rss, rss_url,
                            category, validation_status
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        source['source_id'],
                        source['name'],
                        source['url'],
                        source['type'],
                        1,  # has_rss
                        source['rss_url'],
                        source['category'],
                        'active'
                    ))
                added += 1
                logger.info(f"‚úÖ Added: {source['name']}")
            else:
                # –û–±–Ω–æ–≤–ª—è–µ–º RSS URL –µ—Å–ª–∏ –æ–Ω –ø—É—Å—Ç–æ–π
                if not existing[1] and source['rss_url']:
                    with db.get_connection() as conn:
                        conn.execute("""
                            UPDATE sources 
                            SET rss_url = ?, has_rss = 1
                            WHERE source_id = ?
                        """, (source['rss_url'], source['source_id']))
                    updated += 1
                    logger.info(f"üìù Updated RSS for: {source['name']}")
                else:
                    logger.debug(f"‚è≠Ô∏è Exists: {source['name']}")
                    
        except Exception as e:
            logger.error(f"Error processing {source['name']}: {e}")
    
    # –¢–∞–∫–∂–µ –æ–±–Ω–æ–≤–ª—è–µ–º tracking_sources.json
    tracking_file = Path(__file__).parent.parent / 'data' / 'tracking_sources.json'
    
    # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
    if tracking_file.exists():
        with open(tracking_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = {
            "tracking_sources": [],
            "settings": {
                "default_limit": 20,
                "scan_interval_hours": 6,
                "export_after_scan": False,
                "tag_group_prefix": "tracking_"
            }
        }
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    existing_ids = {s['source_id'] for s in data['tracking_sources']}
    
    for source in sources:
        if source['source_id'] not in existing_ids:
            data['tracking_sources'].append(source)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with open(tracking_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    logger.info(f"  ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –ë–î: {added}")
    logger.info(f"  üìù –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ –ë–î: {updated}")
    logger.info(f"  üìÑ –û–±–Ω–æ–≤–ª–µ–Ω —Ñ–∞–π–ª: {tracking_file}")
    logger.info(f"  üìö –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(sources)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    categories = {}
    for source in sources:
        cat = source['category']
        categories[cat] = categories.get(cat, 0) + 1
    
    logger.info(f"\nüìÇ –ö–ê–¢–ï–ì–û–†–ò–ò:")
    for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  {cat}: {count}")
    
    logger.info(f"\nüí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    logger.info(f"  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ RSS URLs –∫–æ–º–∞–Ω–¥–æ–π:")
    logger.info(f"     python scripts/tracking_manager.py --scan --scan-limit 5")
    logger.info(f"  2. –ò—Å–ø—Ä–∞–≤—å—Ç–µ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–µ RSS URLs –≤ –ë–î")
    logger.info(f"  3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª:")
    logger.info(f"     python scripts/tracking_manager.py --full-cycle")


if __name__ == '__main__':
    main()