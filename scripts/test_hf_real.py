import re
import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
with open("../services/source_extractors.json", "r") as f:
    config = json.load(f)

# –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
with open("content_huggingface.md", "r") as f:
    content = f.read()

hf_config = config["extractors"]["huggingface"]
patterns = hf_config["patterns"]
exclude_urls = hf_config["exclude_urls"]

print("üß™ –†–ï–ê–õ–¨–ù–´–ô –¢–ï–°–¢ HUGGING FACE")
print("=" * 60)

# –ò—â–µ–º URL
url_pattern = patterns[0]
urls = re.findall(url_pattern, content)

print(f"–ù–∞–π–¥–µ–Ω–æ {len(urls)} URL")

# –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
articles = []
for url in urls:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    skip = False
    for exclude in exclude_urls:
        if exclude.endswith('$'):
            # –≠—Ç–æ regex –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤
            if re.search(exclude, url):
                skip = True
                break
        elif exclude in url:
            skip = True
            break
    
    if skip:
        continue
    
    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–µ–¥ URL
    url_pos = content.find(url)
    if url_pos > 0:
        context = content[max(0, url_pos-500):url_pos]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_match = re.search(r'\*\*([^*]+)\*\*', context)
        if title_match:
            title = title_match.group(1).strip()
            articles.append((title, url))

# –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
seen = set()
unique_articles = []
for title, url in articles:
    if url not in seen:
        seen.add(url)
        unique_articles.append((title, url))

print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_articles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π:\n")
for i, (title, url) in enumerate(unique_articles[:15], 1):
    print(f"{i}. {title[:70]}")
    print(f"   {url}")
