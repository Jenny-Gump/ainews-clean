#!/usr/bin/env python3
"""
AI News Parser - Single Pipeline System
–ï–¥–∏–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± –ò–ò
"""
import argparse
import sys
import asyncio
import signal
from datetime import datetime
from dotenv import load_dotenv
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

from core.database import Database
from core.config import Config
from core.single_pipeline import SingleArticlePipeline
from app_logging import configure_logging, get_logger, LogContext
from services.rss_discovery import ExtractRSSDiscovery
from change_tracking import ChangeMonitor

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è graceful shutdown
shutdown_requested = False

def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown"""
    global shutdown_requested
    shutdown_requested = True
    logger = get_logger('core.main')
    logger.info("\n‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â—É—é –æ–ø–µ—Ä–∞—Ü–∏—é...")
    sys.exit(0)

def parse_arguments():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(
        description='AI News Parser - Single Pipeline System',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:

üì° RSS Discovery (–ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π):
  python core/main.py --rss-discover

üöÄ Single Pipeline (–æ–±—Ä–∞–±–æ—Ç–∫–∞ 1 —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∞–∑—ã):
  python core/main.py --single-pipeline

üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏:
  python core/main.py --process-article ARTICLE_ID

üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
  python core/main.py --stats
  python core/main.py --list-sources

üîç Change Tracking (–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π):
  python core/main.py --change-tracking --scan --limit 5
  python core/main.py --change-tracking --stats
  python core/main.py --change-tracking --export

–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô WORKFLOW:
  1. python core/main.py --rss-discover  # –ù–∞–π—Ç–∏ –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏
  2. python core/main.py --single-pipeline  # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å 1 —Å—Ç–∞—Ç—å—é
  3. –ü–æ–≤—Ç–æ—Ä–∏—Ç—å —à–∞–≥ 2 –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–µ–¥—É—é—â–∏—Ö —Å—Ç–∞—Ç–µ–π
  
WORKFLOW –° CHANGE TRACKING:
  1. python core/main.py --change-tracking --scan  # –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
  2. python core/main.py --change-tracking --export  # –≠–∫—Å–ø–æ—Ä—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
  3. python core/main.py --single-pipeline  # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å
        """
    )
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
    mode_group = parser.add_mutually_exclusive_group()
    
    mode_group.add_argument(
        '--rss-discover',
        action='store_true',
        help='Phase 1: –ù–∞–π—Ç–∏ –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ RSS –ª–µ–Ω—Ç'
    )
    
    mode_group.add_argument(
        '--single-pipeline',
        action='store_true',
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –û–î–ù–£ —Å—Ç–∞—Ç—å—é —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∞–∑—ã (2-5)'
    )
    
    mode_group.add_argument(
        '--process-article',
        type=str,
        metavar='ARTICLE_ID',
        help='–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å—Ç–∞—Ç—å—é –ø–æ ID'
    )
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
    mode_group.add_argument(
        '--stats',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã'
    )
    
    mode_group.add_argument(
        '--list-sources',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤'
    )
    
    mode_group.add_argument(
        '--cleanup',
        action='store_true',
        help='–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Å—Ç–∞—Ç—å–∏ (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)'
    )
    
    # Change Tracking commands
    mode_group.add_argument(
        '--change-tracking',
        action='store_true',
        help='–†–µ–∂–∏–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö'
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument(
        '--days',
        type=int,
        default=30,
        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è cleanup (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 30)'
    )
    
    parser.add_argument(
        '--limit',
        type=int,
        default=5,
        help='–õ–∏–º–∏—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)'
    )
    
    # Change tracking sub-commands
    parser.add_argument(
        '--scan',
        action='store_true',
        help='–°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    parser.add_argument(
        '--complete-scan',
        action='store_true',
        help='–ó–∞–≤–µ—Ä—à–∏—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –Ω–µ–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    parser.add_argument(
        '--export',
        action='store_true',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    parser.add_argument(
        '--tracking-stats',
        action='store_true', 
        help='–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=3,
        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)'
    )
    
    parser.add_argument(
        '--extract-urls',
        action='store_true',
        help='–ò–∑–≤–ª–µ—á—å URL —Å—Ç–∞—Ç–µ–π –∏–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    parser.add_argument(
        '--show-new-urls',
        action='store_true',
        help='–ü–æ–∫–∞–∑–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –Ω–æ–≤—ã–µ URL (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    parser.add_argument(
        '--export-articles',
        action='store_true',
        help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–µ URL –≤ —Ç–∞–±–ª–∏—Ü—É articles (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å --change-tracking)'
    )
    
    return parser.parse_args()


async def run_rss_discovery():
    """Phase 1: RSS Discovery - –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    logger = get_logger('core.main')
    
    with LogContext.operation("rss_discovery", phase=1):
        logger.info("üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –∏–∑ RSS –ª–µ–Ω—Ç...")
        
        discovery = ExtractRSSDiscovery()
        stats = await discovery.discover_from_sources()
        
        logger.info(f"‚úÖ RSS Discovery –∑–∞–≤–µ—Ä—à–µ–Ω: {stats}")
        return stats


async def run_single_pipeline():
    """–ó–∞–ø—É—Å–∫ Single Pipeline - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –û–î–ù–û–ô —Å—Ç–∞—Ç—å–∏ —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∞–∑—ã"""
    logger = get_logger('core.main')
    
    with LogContext.operation("single_pipeline", phase="all"):
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ Single Pipeline (1 —Å—Ç–∞—Ç—å—è —á–µ—Ä–µ–∑ –≤—Å–µ —Ñ–∞–∑—ã)")
        
        pipeline = SingleArticlePipeline()
        result = await pipeline.run_pipeline()
        
        if result.get('success'):
            logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {result.get('article_id')}")
        elif result.get('error') == 'No pending articles':
            logger.info("üì≠ –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–≤—Å–µ pending —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã)")
        else:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.get('error')}")
        
        return result


async def process_specific_article(article_id: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –ø–æ ID"""
    logger = get_logger('core.main')
    
    with LogContext.operation("process_specific", article_id=article_id):
        logger.info(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏: {article_id}")
        
        pipeline = SingleArticlePipeline()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—å—é –∏–∑ –ë–î
        db = Database()
        with db.get_connection() as conn:
            cursor = conn.execute(
                "SELECT * FROM articles WHERE article_id = ?",
                (article_id,)
            )
            row = cursor.fetchone()
            
            if not row:
                logger.error(f"‚ùå –°—Ç–∞—Ç—å—è {article_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return {"success": False, "error": "Article not found"}
            
            article = dict(row)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω
        result = await pipeline.process_single_article(article)
        
        if result['success']:
            logger.info(f"‚úÖ –°—Ç–∞—Ç—å—è {article_id} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {article_id}: {result.get('error')}")
        
        return result


def show_stats():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
    logger = get_logger('core.main')
    db = Database()
    
    with db.get_connection() as conn:
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—å—è–º
        cursor = conn.execute("""
            SELECT 
                content_status,
                COUNT(*) as count
            FROM articles
            GROUP BY content_status
            ORDER BY count DESC
        """)
        
        logger.info("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–¢–ê–¢–ï–ô:")
        logger.info("-" * 40)
        total = 0
        for row in cursor:
            status = row['content_status'] or 'unknown'
            count = row['count']
            total += count
            emoji = {
                'pending': '‚è≥',
                'parsed': 'üìÑ',
                'published': '‚úÖ',
                'failed': '‚ùå',
                'completed': '‚úÖ'
            }.get(status, '‚ùì')
            logger.info(f"{emoji} {status:15} {count:5} —Å—Ç–∞—Ç–µ–π")
        logger.info("-" * 40)
        logger.info(f"üìö –í–°–ï–ì–û:           {total:5} —Å—Ç–∞—Ç–µ–π\n")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        cursor = conn.execute("""
            SELECT 
                s.name,
                s.source_id,
                COUNT(a.article_id) as article_count,
                MAX(a.created_at) as last_article
            FROM sources s
            LEFT JOIN articles a ON s.source_id = a.source_id
            GROUP BY s.source_id
            ORDER BY article_count DESC
            LIMIT 10
        """)
        
        logger.info("üì° –¢–û–ü-10 –ò–°–¢–û–ß–ù–ò–ö–û–í:")
        logger.info("-" * 60)
        logger.info(f"{'–ò—Å—Ç–æ—á–Ω–∏–∫':<30} {'–°—Ç–∞—Ç–µ–π':>10} {'–ü–æ—Å–ª–µ–¥–Ω—è—è':<20}")
        logger.info("-" * 60)
        for row in cursor:
            name = row['name'][:30]
            count = row['article_count']
            last = row['last_article'] or 'Never'
            if last != 'Never':
                last = last.split('.')[0]  # –£–±–∏—Ä–∞–µ–º –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã
            logger.info(f"{name:<30} {count:>10} {last:<20}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ–¥–∏–∞
        cursor = conn.execute("""
            SELECT 
                status,
                COUNT(*) as count
            FROM media_files
            GROUP BY status
        """)
        
        media_stats = list(cursor)
        if media_stats:
            logger.info("\nüñºÔ∏è –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–î–ò–ê–§–ê–ô–õ–û–í:")
            logger.info("-" * 40)
            for row in media_stats:
                status = row['status']
                count = row['count']
                logger.info(f"  {status:15} {count:5} —Ñ–∞–π–ª–æ–≤")
        
        # WordPress —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor = conn.execute("""
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN published_to_wp = 1 THEN 1 ELSE 0 END) as published
            FROM wordpress_articles
        """)
        
        wp_stats = cursor.fetchone()
        if wp_stats and wp_stats['total'] > 0:
            logger.info(f"\nüìù WORDPRESS:")
            logger.info("-" * 40)
            logger.info(f"  –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ:    {wp_stats['total']:5} —Å—Ç–∞—Ç–µ–π")
            logger.info(f"  –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ:    {wp_stats['published']:5} —Å—Ç–∞—Ç–µ–π")


def show_sources():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    db = Database()
    
    with db.get_connection() as conn:
        cursor = conn.execute("""
            SELECT 
                source_id,
                name,
                url,
                category,
                (SELECT COUNT(*) FROM articles WHERE source_id = s.source_id) as article_count
            FROM sources s
            ORDER BY category, name
        """)
        
        logger.info("\nüì° –ò–°–¢–û–ß–ù–ò–ö–ò –ù–û–í–û–°–¢–ï–ô:")
        logger.info("=" * 80)
        
        current_category = None
        for row in cursor:
            if row['category'] != current_category:
                current_category = row['category']
                logger.info(f"\n{current_category or 'Uncategorized'}:")
                logger.info("-" * 80)
            
            logger.info(f"üì° {row['name']:<30} [{row['article_count']:>3} —Å—Ç–∞—Ç–µ–π]")
            logger.info(f"   {row['url'][:70]}")


def cleanup_old_articles(days: int = 30):
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π"""
    logger = get_logger('core.main')
    db = Database()
    
    with db.get_connection() as conn:
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ —É–¥–∞–ª–∏–º
        cursor = conn.execute("""
            SELECT COUNT(*) as count
            FROM articles
            WHERE created_at < datetime('now', ? || ' days')
        """, (-days,))
        
        count = cursor.fetchone()['count']
        
        if count == 0:
            logger.info(f"–ù–µ—Ç —Å—Ç–∞—Ç–µ–π —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Ç–∞—Ç—å–∏
        conn.execute("""
            DELETE FROM articles
            WHERE created_at < datetime('now', ? || ' days')
        """, (-days,))
        
        # –£–¥–∞–ª—è–µ–º –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏–µ –º–µ–¥–∏–∞—Ñ–∞–π–ª—ã
        conn.execute("""
            DELETE FROM media_files
            WHERE article_id NOT IN (SELECT article_id FROM articles)
        """)
        
        # –£–¥–∞–ª—è–µ–º –æ—Å–∏—Ä–æ—Ç–µ–≤—à–∏–µ WordPress —Å—Ç–∞—Ç—å–∏
        conn.execute("""
            DELETE FROM wordpress_articles
            WHERE article_id NOT IN (SELECT article_id FROM articles)
        """)
        
        conn.commit()
        
        # VACUUM –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        conn.execute("VACUUM")
        
        logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {count} —Å—Ç–∞—Ç–µ–π —Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π")
        logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {count} —Å—Ç–∞—Ç–µ–π")


async def run_change_tracking(args):
    """–ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    logger = get_logger('core.main')
    
    if args.scan or args.complete_scan:
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        scan_type = "complete" if args.complete_scan else "regular"
        with LogContext.operation("change_tracking_scan", scan_type=scan_type):
            
            if args.complete_scan:
                logger.info("üéØ –ó–∞–≤–µ—Ä—à–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ - —Ç–æ–ª—å–∫–æ –Ω–µ–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏...")
                monitor = ChangeMonitor()
                results = await monitor.scan_sources_batch(
                    batch_size=args.batch_size,
                    only_unscanned=True
                )
            else:
                logger.info("üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è...")
                monitor = ChangeMonitor()
                results = await monitor.scan_sources_batch(
                    batch_size=args.batch_size,
                    limit=args.limit
                )
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–Ø:")
            logger.info("=" * 60)
            logger.info(f"  üìã –í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ: {results['total']}")
            logger.info(f"  üÜï –ù–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü:   {results['new']}")
            logger.info(f"  üîÑ –ò–∑–º–µ–Ω–µ–Ω–∏–π:       {results['changed']}")
            logger.info(f"  ‚ö™ –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π:   {results['unchanged']}")
            logger.info(f"  ‚ùå –û—à–∏–±–æ–∫:         {results['errors']}")
            
            # –î–µ—Ç–∞–ª–∏ –Ω–æ–≤—ã—Ö –∏ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö
            new_and_changed = [
                d for d in results['details'] 
                if d['status'] in ['new', 'changed']
            ]
            
            if new_and_changed:
                logger.info(f"\nüî• –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ({len(new_and_changed)}):")
                logger.info("-" * 60)
                for i, detail in enumerate(new_and_changed[:10], 1):
                    status_icon = 'üÜï' if detail['status'] == 'new' else 'üîÑ'
                    url_short = detail['url'][:50] + '...' if len(detail['url']) > 50 else detail['url']
                    logger.info(f"  {i:2}. {status_icon} {url_short}")
                    if detail.get('article_id'):
                        logger.info(f"      üìù ID: {detail['article_id']}")
                
                if len(new_and_changed) > 10:
                    logger.info(f"      ... –∏ –µ—â—ë {len(new_and_changed) - 10}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏
            errors = [d for d in results['details'] if d['status'] == 'error']
            if errors:
                logger.info(f"\n‚ùå –û–®–ò–ë–ö–ò ({len(errors)}):")
                for i, detail in enumerate(errors[:5], 1):
                    url_short = detail['url'][:50] + '...' if len(detail['url']) > 50 else detail['url']
                    error_short = detail.get('error', '')[:80] + '...' if len(detail.get('error', '')) > 80 else detail.get('error', '')
                    logger.info(f"  {i}. {url_short}")
                    logger.info(f"     üí• {error_short}")
    
    elif args.export:
        # –≠–∫—Å–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω
        with LogContext.operation("change_tracking_export"):
            logger.info("üì§ –≠–∫—Å–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω...")
            
            monitor = ChangeMonitor()
            changed_articles = monitor.get_changed_articles()
            
            if not changed_articles:
                logger.info("‚ÑπÔ∏è –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
                return
            
            logger.info(f"üîÑ –ù–∞–π–¥–µ–Ω–æ {len(changed_articles)} —Å—Ç–∞—Ç–µ–π —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏:")
            for article in changed_articles[:5]:
                logger.info(f"  üìÑ {article['title'][:60]}...")
                logger.info(f"      üåê {article['url'][:70]}...")
            
            if len(changed_articles) > 5:
                logger.info(f"  ... –∏ –µ—â—ë {len(changed_articles) - 5}")
            
            # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç
            logger.info("‚ö†Ô∏è –≠–∫—Å–ø–æ—Ä—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")
            logger.info("üí° –°—Ç–∞—Ç—å–∏ –æ—Å—Ç–∞—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü–µ tracked_articles")
    
    elif args.tracking_stats or not (args.scan or args.complete_scan or args.export or args.extract_urls or args.show_new_urls or args.export_articles):
        # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
        with LogContext.operation("change_tracking_stats"):
            logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–Ø –ò–ó–ú–ï–ù–ï–ù–ò–ô")
            logger.info("=" * 60)
            
            monitor = ChangeMonitor()
            stats = monitor.get_tracking_stats()
            
            if 'error' in stats:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {stats['error']}")
                return
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            logger.info(f"üìã –í—Å–µ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è: {stats.get('total_tracked', 0)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –ü–æ —Å—Ç–∞—Ç—É—Å–∞–º
            if stats.get('by_status'):
                logger.info(f"\nüìà –ü–û –°–¢–ê–¢–£–°–ê–ú:")
                status_icons = {
                    'new': 'üÜï',
                    'changed': 'üîÑ', 
                    'unchanged': '‚ö™',
                    'unknown': '‚ùì'
                }
                for status, count in stats['by_status'].items():
                    icon = status_icons.get(status, 'üìÑ')
                    logger.info(f"  {icon} {status.upper():12}: {count:4} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (—Ç–æ–ø 10)
            if stats.get('by_source'):
                logger.info(f"\nüåê –¢–û–ü –ò–°–¢–û–ß–ù–ò–ö–ò:")
                sorted_sources = sorted(
                    stats['by_source'].items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )
                for i, (source, count) in enumerate(sorted_sources[:10], 1):
                    logger.info(f"  {i:2}. {source:25} {count:3} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if stats.get('recent_changes'):
                logger.info(f"\nüî• –ü–û–°–õ–ï–î–ù–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø:")
                for i, change in enumerate(stats['recent_changes'][:5], 1):
                    status_icon = 'üÜï' if change['status'] == 'new' else 'üîÑ'
                    url_short = change['url'][:45] + '...' if len(change['url']) > 45 else change['url']
                    logger.info(f"  {i}. {status_icon} {url_short}")
                    logger.info(f"     ‚è∞ {change['checked']}")
    
    elif args.extract_urls:
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        with LogContext.operation("change_tracking_extract_urls"):
            logger.info("üîó –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL —Å—Ç–∞—Ç–µ–π –∏–∑ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
            
            monitor = ChangeMonitor()
            results = await monitor.extract_urls_from_all_tracked(limit=args.limit)
            
            logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø URL:")
            logger.info("=" * 60)
            logger.info(f"  üìã –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {results['processed']}")
            logger.info(f"  üîó –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö URL:  {results['new_urls']}")
            
            if results.get('error'):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {results['error']}")
            else:
                logger.info(f"‚úÖ {results.get('message', '–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ')}")
    
    elif args.show_new_urls:
        # –ü–æ–∫–∞–∑–∞—Ç—å –Ω–æ–≤—ã–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ URL
        with LogContext.operation("change_tracking_show_urls"):
            logger.info("üìã –ù–û–í–´–ï –ù–ê–ô–î–ï–ù–ù–´–ï URL")
            logger.info("=" * 80)
            
            monitor = ChangeMonitor()
            url_stats = monitor.get_url_extraction_stats()
            
            if url_stats.get('error'):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {url_stats['error']}")
                return
            
            logger.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê URL:")
            logger.info(f"  üîó –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ:      {url_stats.get('total_urls', 0)}")
            logger.info(f"  üÜï –ù–æ–≤—ã—Ö:              {url_stats.get('new_urls', 0)}")
            logger.info(f"  üì§ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ:     {url_stats.get('exported_urls', 0)}")
            logger.info(f"  ‚è≥ –û–∂–∏–¥–∞–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:   {url_stats.get('pending_export', 0)}")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ø –¥–æ–º–µ–Ω–æ–≤
            if url_stats.get('top_domains'):
                logger.info(f"\nüåê –¢–û–ü –î–û–ú–ï–ù–´:")
                for i, (domain, count) in enumerate(list(url_stats['top_domains'].items())[:10], 1):
                    logger.info(f"  {i:2}. {domain:25} {count:3} URL")
            
            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ URL
            if url_stats.get('recent_urls'):
                logger.info(f"\nüî• –ü–û–°–õ–ï–î–ù–ò–ï –ù–ê–ô–î–ï–ù–ù–´–ï URL:")
                for i, url_data in enumerate(url_stats['recent_urls'][:5], 1):
                    title = url_data['title'][:50] + '...' if len(url_data['title']) > 50 else url_data['title']
                    logger.info(f"  {i}. {title}")
                    logger.info(f"     üåê {url_data['url'][:70]}...")
                    logger.info(f"     üìÖ {url_data['discovered']}")
    
    elif args.export_articles:
        # –≠–∫—Å–ø–æ—Ä—Ç –Ω–æ–≤—ã—Ö URL –≤ —Ç–∞–±–ª–∏—Ü—É articles
        with LogContext.operation("change_tracking_export_articles"):
            logger.info("üì§ –≠–∫—Å–ø–æ—Ä—Ç –Ω–æ–≤—ã—Ö URL –≤ —Ç–∞–±–ª–∏—Ü—É articles...")
            
            monitor = ChangeMonitor()
            results = monitor.export_new_urls_to_articles(limit=args.limit)
            
            logger.info(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–ö–°–ü–û–†–¢–ê:")
            logger.info("=" * 60)
            logger.info(f"  üì§ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ:     {results['exported']}")
            
            if 'total_available' in results:
                logger.info(f"  üìã –ë—ã–ª–æ –¥–æ—Å—Ç—É–ø–Ω–æ:      {results['total_available']}")
            
            if results.get('error'):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {results['error']}")
            else:
                logger.info(f"‚úÖ {results.get('message', '–≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω')}")
                
                if results['exported'] > 0:
                    logger.info(f"\nüí° –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
                    logger.info(f"   –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
                    logger.info(f"   python core/main.py --single-pipeline")
    
    else:
        logger.info("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è --change-tracking")
        logger.info("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: --scan, --complete-scan, --export, --tracking-stats,")
        logger.info("    --extract-urls, --show-new-urls, –∏–ª–∏ --export-articles")


async def run_monitoring(rss_url: str, limit: int = 20):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–µ"""
    logger = get_logger('core.main')
    
    with LogContext.operation("change_monitoring", source=rss_url):
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {rss_url}")
        
        monitor = ChangeMonitor()
        results = await monitor.scan_source(rss_url, limit)
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"\nüîç –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê: {rss_url}")
        logger.info("=" * 80)
        
        logger.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"  ‚úÖ –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π:     {len(results['new'])}")
        logger.info(f"  üîÑ –ò–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö:       {len(results['changed'])}")
        logger.info(f"  ‚ö™ –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π:    {len(results['unchanged'])}")
        logger.info(f"  ‚ùå –û—à–∏–±–æ–∫:          {len(results['errors'])}")
        
        if results['new']:
            logger.info(f"\nüì∞ –ù–û–í–´–ï –°–¢–ê–¢–¨–ò ({len(results['new'])}):")
            logger.info("-" * 80)
            for i, article in enumerate(results['new'][:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                title = article['title'][:70]
                article_id = article['article_id'][:8]
                logger.info(f"{i:2}. [{article_id}] {title}")
            
            if len(results['new']) > 10:
                logger.info(f"    ... –∏ –µ—â—ë {len(results['new']) - 10} —Å—Ç–∞—Ç–µ–π")
        
        if results['changed']:
            logger.info(f"\nüîÑ –ò–ó–ú–ï–ù–ï–ù–ù–´–ï –°–¢–ê–¢–¨–ò ({len(results['changed'])}):")
            logger.info("-" * 80)
            for i, article in enumerate(results['changed'][:5], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                title = article['title'][:70]
                article_id = article['article_id'][:8]
                logger.info(f"{i:2}. [{article_id}] {title}")
                
            if len(results['changed']) > 5:
                logger.info(f"    ... –∏ –µ—â—ë {len(results['changed']) - 5} —Å—Ç–∞—Ç–µ–π")
        
        if results['errors']:
            logger.info(f"\n‚ùå –û–®–ò–ë–ö–ò ({len(results['errors'])}):")
            logger.info("-" * 80)
            for error in results['errors'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                url = error.get('url', 'unknown')[:50]
                error_msg = error.get('error', 'unknown')[:50]
                logger.info(f"  ‚Ä¢ {url}: {error_msg}")
        
        total_detected = len(results['new']) + len(results['changed'])
        if total_detected > 0:
            logger.info(f"\nüí° –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
            logger.info(f"   –î–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –æ—Å–Ω–æ–≤–Ω—É—é –ë–î –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
            logger.info(f"   python core/main.py --export-tracked --all")
        
        logger.info(f"‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {total_detected} –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞–π–¥–µ–Ω–æ")
        return results


async def export_tracked_articles(export_all: bool = False, article_ids: str = None):
    """–≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç–∞—Ç–µ–π –≤ –æ—Å–Ω–æ–≤–Ω—É—é –ë–î"""
    logger = get_logger('core.main')
    
    with LogContext.operation("export_tracked"):
        logger.info("üì§ –ù–∞—á–∏–Ω–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç–∞—Ç–µ–π...")
        
        monitor = ChangeMonitor()
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ ID
        ids_list = None
        if article_ids:
            ids_list = [id.strip() for id in article_ids.split(',')]
            logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π: {len(ids_list)} ID")
        elif export_all:
            logger.info("–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π")
        else:
            # –ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞
            pending = monitor.get_pending_export(limit=20)
            if not pending:
                logger.info("üì≠ –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
                return
            
            logger.info(f"\nüìã –°–¢–ê–¢–¨–ò –î–û–°–¢–£–ü–ù–´–ï –î–õ–Ø –≠–ö–°–ü–û–†–¢–ê ({len(pending)}):")
            logger.info("=" * 80)
            for i, article in enumerate(pending, 1):
                title = article['title'][:60]
                article_id = article['article_id'][:8]
                status = article['change_status']
                logger.info(f"{i:2}. [{article_id}] {status:8} {title}")
            
            logger.info(f"\nüí° –î–õ–Ø –≠–ö–°–ü–û–†–¢–ê –ò–°–ü–û–õ–¨–ó–£–ô–¢–ï:")
            logger.info(f"   --export-tracked --all  (—ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ)")
            logger.info(f"   --export-tracked --ids ID1,ID2,ID3  (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ)")
            return
        
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç
        results = await monitor.export_to_main(ids_list)
        
        # –ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        logger.info(f"\nüì§ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–ö–°–ü–û–†–¢–ê:")
        logger.info("=" * 50)
        logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ:     {results['total_exported']} —Å—Ç–∞—Ç–µ–π")
        logger.info(f"‚ö†Ô∏è –î—É–±–ª–µ–π –ø—Ä–æ–ø—É—â–µ–Ω–æ:   {len(results['duplicates'])} —Å—Ç–∞—Ç–µ–π")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫:            {len(results['errors'])} —Å—Ç–∞—Ç–µ–π")
        
        if results['exported']:
            logger.info(f"\nüì∞ –≠–ö–°–ü–û–†–¢–ò–†–û–í–ê–ù–ù–´–ï –°–¢–ê–¢–¨–ò:")
            logger.info("-" * 60)
            for article in results['exported'][:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                title = article['title'][:50]
                new_id = article['new_id'][:8]
                logger.info(f"  [{new_id}] {title}")
            
            if len(results['exported']) > 10:
                logger.info(f"    ... –∏ –µ—â—ë {len(results['exported']) - 10} —Å—Ç–∞—Ç–µ–π")
        
        if results['duplicates']:
            logger.info(f"\n‚ö†Ô∏è –î–£–ë–õ–ò –ü–†–û–ü–£–©–ï–ù–´:")
            logger.info("-" * 60)
            for dup in results['duplicates'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                title = dup['title'][:50]
                existing_id = dup['existing_id'][:8]
                logger.info(f"  [{existing_id}] {title}")
        
        if results['errors']:
            logger.info(f"\n‚ùå –û–®–ò–ë–ö–ò:")
            logger.info("-" * 60)
            for error in results['errors'][:3]:
                title = error.get('title', 'unknown')[:40]
                error_msg = error.get('error', 'unknown')[:30]
                logger.info(f"  {title}: {error_msg}")
        
        if results['total_exported'] > 0:
            logger.info(f"\nüí° –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì:")
            logger.info(f"   –î–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:")
            logger.info(f"   python core/main.py --single-pipeline")
        
        logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {results['total_exported']} —Å—Ç–∞—Ç–µ–π")
        return results


def show_tracking_stats():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"""
    monitor = ChangeMonitor()
    stats = monitor.get_tracking_stats()
    
    logger.info(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–Ø:")
    logger.info("=" * 50)
    logger.info(f"üìö –í—Å–µ–≥–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è:   {stats['total_tracked']} —Å—Ç–∞—Ç–µ–π")
    logger.info(f"üì§ –û–∂–∏–¥–∞—é—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞:      {stats['pending_export']} —Å—Ç–∞—Ç–µ–π")
    
    if stats['by_status']:
        logger.info(f"\nüîç –ü–û –°–¢–ê–¢–£–°–£ –ò–ó–ú–ï–ù–ï–ù–ò–ô:")
        logger.info("-" * 40)
        for status, count in stats['by_status'].items():
            emoji = {
                'new': 'üÜï',
                'changed': 'üîÑ',
                'unchanged': '‚ö™'
            }.get(status, '‚ùì')
            logger.info(f"{emoji} {status:12} {count:5} —Å—Ç–∞—Ç–µ–π")
    
    if stats['by_source']:
        logger.info(f"\nüì° –ü–û –ò–°–¢–û–ß–ù–ò–ö–ê–ú:")
        logger.info("-" * 50)
        for source_id, count in sorted(stats['by_source'].items(), 
                                       key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"üì° {source_id:<25} {count:5} —Å—Ç–∞—Ç–µ–π")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞—é—â–∏—Ö —ç–∫—Å–ø–æ—Ä—Ç–∞
    if stats['pending_export'] > 0:
        pending = monitor.get_pending_export(limit=5)
        logger.info(f"\nüìã –ü–û–°–õ–ï–î–ù–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø (—Ç–æ–ø-5):")
        logger.info("-" * 70)
        for article in pending:
            title = article['title'][:50]
            status = article['change_status']
            last_checked = article['last_checked'][:19] if article['last_checked'] else 'unknown'
            logger.info(f"üîÑ {status:8} {last_checked} {title}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    configure_logging()
    logger = get_logger('core.main')
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    args = parse_arguments()
    
    try:
        logger.info("üöÄ AI News Parser - Single Pipeline System")
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥
        if args.rss_discover:
            await run_rss_discovery()
            
        elif args.single_pipeline:
            await run_single_pipeline()
            
        elif args.process_article:
            await process_specific_article(args.process_article)
            
        elif args.stats:
            show_stats()
            
        elif args.list_sources:
            show_sources()
            
        elif args.cleanup:
            cleanup_old_articles(args.days)
            
        elif args.change_tracking:
            await run_change_tracking(args)
            
        else:
            logger.info("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω–∞ –∫–æ–º–∞–Ω–¥–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("‚ö†Ô∏è –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())