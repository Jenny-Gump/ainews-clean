#!/usr/bin/env python3
"""
Batch Testing Script - Round 5
–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: robotics companies
"""

import requests
import json
import sys
import re
import time
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ —Å–∏—Å—Ç–µ–º–µ
sys.path.insert(0, str(Path(__file__).parent.parent))
from core.config import Config

config = Config()

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞)
SOURCES_TO_TEST = [
    {
        "id": "fanuc",
        "name": "FANUC America News",
        "url": "https://www.fanucamerica.com/news-resources/articles/all"
    },
    {
        "id": "kuka",
        "name": "KUKA Robotics News",
        "url": "https://www.kuka.com/en-us/company/press/news"
    },
    {
        "id": "kinova",
        "name": "Kinova Robotics Press",
        "url": "https://www.kinovarobotics.com/press"
    },
    {
        "id": "doosan_robotics",
        "name": "Doosan Robotics News",
        "url": "https://www.doosanrobotics.com/en/about/promotion/news/"
    },
    {
        "id": "manus",
        "name": "Manus Blog",
        "url": "https://manus.im/blog"
    }
]

def fetch_content_firecrawl(url: str, timeout: int = 120000):
    """–ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π markdown —á–µ—Ä–µ–∑ Firecrawl API"""
    api_url = "https://api.firecrawl.dev/v1/scrape"
    headers = {
        "Authorization": f"Bearer {config.FIRECRAWL_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "url": url,
        "formats": ["markdown"],
        "timeout": timeout
    }
    
    try:
        response = requests.post(api_url, headers=headers, json=data, timeout=150)
        
        if response.status_code == 200:
            result = response.json()
            if result.get("success"):
                markdown = result.get("data", {}).get("markdown", "")
                return markdown, None
            else:
                return None, f"API error: {result}"
        else:
            return None, f"HTTP Error {response.status_code}: {response.text}"
            
    except Exception as e:
        return None, f"Exception: {str(e)}"

def analyze_and_create_patterns(source_id: str, content: str, url: str):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏ —Å–æ–∑–¥–∞–µ—Ç patterns –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    print(f"\nüîç ANALYZING {source_id.upper()}")
    print(f"Content length: {len(content)} characters")
    print("=" * 60)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫  
    domain = url.split('/')[2] if '://' in url else ''
    base_domain = domain.replace('www.', '').replace('.', '\\.')
    
    # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —Å—Å—ã–ª–æ–∫ —Å –¥–æ–º–µ–Ω–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    patterns_to_try = [
        rf'\[([^\]]+)\]\((https://[^)]*{re.escape(domain.replace("www.", ""))}/[^)]+)\)',
        rf'\[([^\]]+)\]\((https://[^)]*{re.escape(domain)}/[^)]+)\)',
    ]
    
    found_links = []
    
    for pattern_template in patterns_to_try:
        matches = list(re.finditer(pattern_template, content, re.IGNORECASE))
        print(f"Pattern search found: {len(matches)} matches")
        
        for match in matches[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
            groups = match.groups()
            if len(groups) >= 2:
                text, link_url = groups[0], groups[1]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                if any(keyword in (text + link_url).lower() for keyword in 
                       ['news', 'article', 'blog', 'post', 'press', 'announcement', 'release']):
                    found_links.append((text, link_url))
                    print(f"  - [{text[:50]}...] -> {link_url}")
    
    if not found_links:
        print("‚ùå No relevant links found")
        return None
        
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º URL patterns
    url_paths = set()
    for text, link_url in found_links:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É URL  
        parts = link_url.split('/')
        if len(parts) >= 4:
            # –°–æ–∑–¥–∞–µ–º pattern –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö URL
            base_pattern = '/'.join(parts[:4])  # https://domain.com/path
            if len(parts) > 4:
                base_pattern += '/' + parts[4] if not re.match(r'^\d+$', parts[4]) else '/[^)]+'
            else:
                base_pattern += '/[^)]+'
            
            # –ó–∞–º–µ–Ω—è–µ–º –¥–æ–º–µ–Ω –Ω–∞ regex pattern
            pattern_url = base_pattern.replace(domain, base_domain)
            url_paths.add(pattern_url)
    
    # –°–æ–∑–¥–∞–µ–º –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—ã–µ patterns
    patterns = []
    for path_pattern in list(url_paths)[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 patterns
        pattern = f"\\]\\(({path_pattern})\\)"
        patterns.append(pattern)
    
    # Title extraction patterns
    title_extractions = [
        "\\[([^\\]]+)\\]\\(",
        "\\*\\*([^*]+)\\*\\*",
        "### ([^\\n]+)",
        "## ([^\\n]+)",
        "^([^\\[\\]\\n]{15,100})$"  # –ß–∏—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    ]
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ patterns
    print(f"\nüß™ TESTING PATTERNS FOR {source_id.upper()}")
    all_matches = []
    
    for i, pattern in enumerate(patterns, 1):
        print(f"Pattern {i}: {pattern}")
        
        try:
            matches = list(re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE))
            print(f"   Found: {len(matches)} matches")
            
            for j, match in enumerate(matches[:5], 1):
                groups = match.groups()
                if len(groups) >= 1:
                    url = groups[0]
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å title
                    title = "No title"
                    full_match = match.group(0)
                    context_start = max(0, match.start() - 100)
                    context = content[context_start:match.start()]
                    
                    for title_pattern in title_extractions:
                        title_match = re.search(title_pattern, full_match + context)
                        if title_match:
                            title = title_match.group(1).strip()
                            break
                    
                    print(f"   {j}. [{title[:60]}{'...' if len(title) > 60 else ''}]")
                    print(f"      URL: {url}")
                    
                    all_matches.append({'title': title, 'url': url})
                    
        except re.error as e:
            print(f"   ‚ùå Regex error: {e}")
    
    if all_matches:
        return {
            "patterns": patterns,
            "title_extractions": title_extractions,
            "matches": all_matches,
            "unique_urls": len(set(match['url'] for match in all_matches))
        }
    
    return None

def create_extractor_config(source_id: str, name: str, url: str, analysis_result: dict):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é extractor'–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
    return {
        "name": name,
        "url": url,
        "patterns": analysis_result["patterns"],
        "title_extraction": analysis_result["title_extractions"],
        "title_cleanup": [
            f"^{name.split()[0]}:\\s*",
            "^\\*\\*|\\*\\*$",
            "\\s+$",
            "\\\\$"
        ],
        "exclude_urls": [
            "/contact", "/about", "/careers", "/pricing", 
            "/page/", "/category/", "/tag/", "/#", "/search"
        ],
        "status": "tested_real"
    }

def update_config_file(source_configs: dict):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç source_extractors.json —Å –Ω–æ–≤—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏"""
    config_path = Path(__file__).parent.parent / 'services' / 'source_extractors.json'
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º extractors
        for source_id, source_config in source_configs.items():
            config_data['extractors'][source_id] = source_config
            print(f"‚úÖ Updated config for {source_id}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Configuration updated in {config_path}")
        
    except Exception as e:
        print(f"‚ùå Failed to update config: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ STARTING BATCH 5 TESTING (ROBOTICS COMPANIES)")
    print(f"Sources to test: {len(SOURCES_TO_TEST)}")
    print("=" * 80)
    
    updated_configs = {}
    
    for i, source in enumerate(SOURCES_TO_TEST, 1):
        print(f"\n\n{'='*80}")
        print(f"TESTING SOURCE {i}/{len(SOURCES_TO_TEST)}: {source['name']}")
        print(f"{'='*80}")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        print(f"üì• Fetching content from {source['url']}")
        content, error = fetch_content_firecrawl(source['url'], timeout=120000)
        
        if error:
            print(f"‚ùå Failed to fetch content: {error}")
            continue
            
        if not content:
            print(f"‚ùå No content received")
            continue
            
        if len(content) < 200:
            print(f"‚ùå Content too short: {len(content)} characters")
            continue
            
        print(f"‚úÖ Content fetched: {len(content)} characters")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        content_file = Path(__file__).parent / f"content_{source['id']}.md"
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"üíæ Content saved to {content_file}")
        
        # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ —Å–æ–∑–¥–∞–µ–º patterns
        analysis_result = analyze_and_create_patterns(source['id'], content, source['url'])
        
        if analysis_result:
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            config_entry = create_extractor_config(
                source['id'], source['name'], source['url'], analysis_result
            )
            updated_configs[source['id']] = config_entry
            
            print(f"\n‚úÖ SUCCESS: {source['id']}")
            print(f"   Patterns: {len(analysis_result['patterns'])}")
            print(f"   Matches found: {len(analysis_result['matches'])}")
            print(f"   Unique URLs: {analysis_result['unique_urls']}")
        else:
            print(f"\n‚ùå FAILED: {source['id']} - No usable patterns found")
        
        # Rate limiting - –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if i < len(SOURCES_TO_TEST):
            print(f"‚è≥ Waiting 5 seconds before next source...")
            time.sleep(5)
    
    # 3. –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
    if updated_configs:
        print(f"\n\nüìù UPDATING CONFIGURATION FILE")
        print(f"Successfully tested sources: {len(updated_configs)}")
        update_config_file(updated_configs)
        
        print(f"\nüéâ BATCH 5 COMPLETED!")
        print(f"‚úÖ Tested sources: {list(updated_configs.keys())}")
    else:
        print(f"\n‚ùå BATCH 5 FAILED - No sources successfully configured")

if __name__ == "__main__":
    main()